#import "@preview/showybox:2.0.4": showybox

#import "@preview/ilm:1.4.1": *
#import "@preview/finite:0.5.0": automaton
#import "@preview/finite:0.5.0"

#set text(font:("Libertinus Serif", "Source Han Serif SC"))

#show heading.where(level: 1): set text(navy.lighten(0%))
#show heading.where(level: 2): set text(navy.lighten(20%))
#show heading.where(level: 3): set text(navy.lighten(40%))

#show ref: it => {
  text(purple, it)
}

#show: ilm.with(
  title: [Computer Vision],
  date: datetime.today(),
  author: "Tianlin Pan",
  table-of-contents: outline(depth: 2),
)

#set heading(numbering: "1.1.1")
#set page(numbering: "1")
#set text(14pt)
#show raw: set text(font: ("Maple Mono NF"), size: 12pt)

#let frameSettings = (
  border-color: navy,
  title-color: navy.lighten(30%),
  body-color: navy.lighten(95%),
  footer-color: navy.lighten(80%)
)

#let frameSettingsEastern = (
  border-color: eastern,
  title-color: eastern.lighten(30%),
  body-color: eastern.lighten(95%),
  footer-color: eastern.lighten(80%)
)

= 数字图像基础
== 数字图像的表示
图像是一个二维函数 $f(x, y)$, 其幅值称为 *强度* 或 *灰度*. 数字图像是由有限数量的元素组成的, 每个元素具有离散的数值. 这些元素称为 *像素* (picture element). 数字图像可以表示为一个二维矩阵, 其中每个元素对应于图像中的一个像素.

== 图像分辨率
- 空间分辨率 (PPI): 每英寸像素数, 描述图像的细节程度. 比如说, 300 PPI 意味着每英寸有 300 个像素, 2 英寸 乘 2 英寸的图像将包含 600 乘 600 个像素. 
- 设备分辨率 (DPI): 每英寸点数, 描述打印机或显示器的输出质量. 比如说, 600 DPI 意味着每英寸可以打印或显示 600 个独立的点.

== 视觉动态范围
视觉动态范围是指人眼能够感知的亮度范围. 人眼可以适应非常宽的亮度范围, 从非常暗的环境 (如月光下) 到非常亮的环境 (如阳光直射). 典型情况下, 人眼可以感知的亮度范围约为 $10^6:1$.

*高动态范围图像的合成*: 通过拍摄同一场景的多张不同曝光的照片, 然后将它们合成为一张高动态范围图像. 这种方法可以捕捉到更多的细节, 特别是在亮部和暗部.

== 数字图像的基本操作
=== 点运算
点运算是指对图像中的每个像素独立进行操作. 常见的点运算包括:
- 亮度调整: 通过增加或减少像素值来调整图像的亮度.
- 对比度调整: 通过拉伸或压缩像素值的范围来调整图像的对比度.

=== 代数运算
代数运算是指对两幅图像的对应像素进行操作. 常见的代数运算包括加法、乘法和减法. 这些操作可以用于图像的融合、差异检测等.

=== 逻辑运算
逻辑运算是指对图像的每个像素进行逻辑运算. 常见的逻辑运算包括与、或、非等. 这些操作可以用于图像的分割和特征提取.

= 图像变换
== 空间域变换
=== 齐次坐标
齐次坐标是对笛卡尔坐标的一种扩展, 允许我们使用矩阵运算来表示各种几何变换. 对于二维空间中的点 $(x, y)$, 其齐次坐标表示为 $(x, y, w)$, 其中 $w$ 是一个非零的缩放因子. 通常情况下, 我们可以将 $w$ 设为 $1$, 这样点 $(x, y)$ 在齐次坐标中表示为 $(x, y, 1)$
=== 欧式变换
欧式变换包括平移和旋转, 它们保持距离和角度不变. 在二维空间中, 欧式变换可以表示为一个 $3 times 3$ 矩阵, 其形式如下:
$
mat(delim: "[", 
cos theta, - sin theta, t_x ;
sin theta, cos theta, t_y ;
0, 0, 1)
$

它的自由度为 $3$, 包括一个旋转角度 $theta$ 和两个平移参数 $t_x, t_y$.

=== 相似变换
相似变换包括欧式变换和缩放, 它们保持形状但不保持大小. 在二维空间中, 相似变换可以表示为一个 $3 times 3$ 矩阵, 其形式如下:
$
mat(delim: "[",
s cos theta, - s sin theta, t_x ;
s sin theta, s cos theta, t_y ;
0, 0, 1)
$

它的自由度为 $4$, 包括一个缩放因子 $s$, 一个旋转角度 $theta$ 和两个平移参数 $t_x, t_y$.


=== 仿射变换
仿射变换包括相似变换和剪切, 它们保持直线和平行关系. 在二维空间中, 仿射变换可以表示为一个 $3 times 3$ 矩阵, 其形式如下:
$
mat(delim: "[",
a_11, a_12, t_x ;
a_21, a_22, t_y ;
0, 0, 1)
$
它的自由度为 $6$, 包括四个线性变换参数 $a_11, a_12, a_21, a_22$ 和两个平移参数 $t_x, t_y$.

=== 投影变换

投影变换包括仿射变换和透视效果, 它们可以改变直线和平行关系. 在二维空间中, 投影变换可以表示为一个 $3 times 3$ 矩阵, 其形式如下:
$
mat(delim: "[",
a_11, a_12, t_x ;
a_21, a_22, t_y ;
a_31, a_32, 1)
$
它的自由度为 $8$, 包括六个线性变换参数和两个平移参数.

=== 3D 空间中的变换的自由度
- 欧式变换: $6$ (3 个平移参数 + 3 个旋转参数)
- 相似变换: $7$ (6 个欧式变换参数 + 1 个缩放参数)
- 仿射变换: $12$ ($4 times 4$ 的矩阵, 但最后一行通常为 $[0, 0, 0, 1]$, 因此有 $12$ 个自由度)
- 投影变换: $15$ ($4 times 4$ 的矩阵, 但最后一行通常为 $[h_{41}, h_{42}, h_{43}, 1]$, 因此有 $15$ 个自由度)

=== 灰度变换
灰度变换是指对图像的灰度值进行非线性变换, 以增强图像的对比度或亮度 (点变换). 常见的灰度变换包括:
- 线性变换: $s = a r + b$, 其中 $a$ 和 $b$ 是常数. 如对比度, 亮度调整.
- 对数变换: $s = c log(1 + r)$, 其中 $c$ 是常数. 适用于增强暗部细节.
- 伽马变换: $s = c r^(gamma)$, 其中 $c$ 和 $gamma$ 是常数. 适用于调整图像的整体亮度.

== 频域变换
用于分析图像中的频率成分, 将图像表示为不同频率和振幅的模式之和.

=== 连续 Fourier 变换
连续 Fourier 变换将空间域中的图像 $f(x, y)$ 转换为频域中的表示 $F(u, v)$:
$
F(u, v) = integral_(-oo)^(oo) integral_(-oo)^(oo) f(x, y) e^(-j 2 pi(u x + v y)) dif x dif y
$

=== 离散 Fourier 变换 (DFT)
离散 Fourier 变换将离散的图像 $f[m, n]$ 转换为频域中的表示 $F[k, l]$:

$
F[k, l] = sum_(m=0)^(M-1) sum_(n=0)^(N-1) f[m, n] e^(-j 2 pi(k m / M + l n / N))
$
= 图像滤波与数字滤波器
== 图像卷积与相关
=== 定义
对于二维离散信号 (图像) $f(m, n)$ 和 $h(m, n)$, 它们的相关 (correlation) 和卷积 (convolution) 定义如下:
- 相关:
$
g(m, n) = sum_(j=-a)^(a) sum_(k=-b)^(b) f(m + j, n + k) h(j, k)
$
- 卷积:
$
g(m, n) = sum_(j=-a)^(a) sum_(k=-b)^(b) f(m - j, n - k) h(j, k)
$
=== 相关和卷积的关系
相关和卷积之间的关系可以通过翻转滤波器 $h(m, n)$ 来表示. 具体来说, 卷积可以看作是相关操作, 但滤波器被翻转了 $180$ 度:

=== 卷积运算的性质
- 交换律: $f * h = h * f$
- 结合律: $f * (h * g) = (f * h) * g$
- 分配律: $f * (h + g) = f * h + f * g$
- 恒等元: $f * delta = f$, 其中 $delta$ 是单位脉冲函数.
- 微分性: $partial / (partial x) (f * h) = (partial f) / (partial x) * h = f * (partial h) / (partial x)$

=== 卷积定理
卷积定理指出, 空间域中的卷积对应于频域中的乘法. 具体来说, 如果 $g(m, n) = f(m, n) * h(m, n)$, 则其 Fourier 变换满足:
$
G(u, v) = F(u, v) H(u, v)
$

== 经典数字滤波器
=== 滤波和滤波器
- 滤波: 通过某种操作来改变图像的频率成分, 以达到增强或抑制某些特征的目的.
- 滤波器: 用于执行滤波操作的工具或算法, 可以是空间域的 (如卷积核) 或频域的 (如频率响应).

=== 图像噪声
1. 高斯噪声: 每一个像素的噪声值 i.i.d. 服从高斯分布, 常见于电子设备产生的噪声.
2. 椒盐噪声: 噪声值随机地取最大值 (灰度值 $=255$, 盐噪声) 或最小值 (灰度值 $=0$, 椒噪声), 常见于传输错误或故障.
3. 泊松噪声: 噪声值服从泊松分布, 常见于光子计数过程, 如低光照条件下的图像采集.

=== 高斯滤波
高斯滤波是一种线性平滑滤波器, 其卷积核由高斯函数定义. 高斯滤波器的形式如下:
$
h(x, y) = (1)/(2 pi sigma^2) e^(-(x^2 + y^2) / (2 sigma^2))
$
在离散形式中, 高斯滤波器可以表示为一个 $m times n$ 的矩阵, 其中每个元素由高斯函数计算得到. 高斯滤波器的标准差 $sigma$ 控制了滤波器的平滑程度, 较大的 $sigma$ 会导致更强的平滑效果.

高斯滤波可以用于图像降噪, 但是在降低噪声的同时, 也会使得图像变得模糊, 特别是边缘部分.

=== 双边滤波
双边滤波在高斯滤波的基础上加入像素值权重项, 既关注像素的位置信息, 同时也考虑像素的灰度 (颜色) 信息. 在像素值权重项中, 像素灰度 (颜色) 越相近, 则权重越大. 

考虑像素 $q$ 的邻域 $S$, 双边滤波的计算公式如下:
$
I^'(q) = (1)/(W) sum_(p in S) I(p) f(||p - q||) g(|I(p) - I(q)|)
$
其中
1. $f(||p - q||)$ 是 *空间权重函数*, 通常采用高斯函数, 用于衡量像素 $p$ 与像素 $q$ 之间的空间距离.
2. $g(|I(p) - I(q)|)$ 是 *像素值权重函数*, 通常也采用高斯函数, 用于衡量像素 $p$ 与像素 $q$ 之间的灰度差异.
3. $W$ 是 *归一化因子*, 用于确保权重和为 $1$.

双边滤波同时关注像素的位置信息和颜色信息, 在滤除噪声的同时, 保留了图像边缘.

=== Wiener 滤波
图像采集过程中造成图像退化, 退化模型为
$
g(x, y) = h(x, y) * f(x, y) + n(x, y)
$
这里的 $h(x, y)$ 是退化函数, $n(x, y)$ 是噪声. 

图像的空间域滤波主要针对加性噪声, 无法处理图像退化. Wiener 滤波的目标是通过已知的退化函数 $h(x, y)$ 和噪声统计特性, 来估计原始图像 $f(x, y)$. 它通过在频域中对图像进行滤波, 以最小化恢复图像与原始图像之间的均方误差. Wiener 滤波器的频率响应可以表示为:
$
H_w(u, v) = (|H(u, v)|^2) / (|H(u, v)|^2 + (S_n(u, v) / S_f(u, v))) * (1 / H(u, v))
$
其中 $H(u, v)$ 是退化函数的频率响应, $S_n(u, v)$ 是噪声的功率谱密度, $S_f(u, v)$ 是原始图像的功率谱密度, 对应的比例系数 $K(u, v) = S_n(u, v) / S_f(u, v)$ 就是信号与噪声的比值 (SNR).


= 形态学与基于深度学习的图像去噪
== 结构元素
结构元素 (structuring element) 是一个小的二值矩阵, 用于定义形态学操作的邻域. 结构元素通常具有对称的形状, 如方形、圆形或十字形. 结构元素的大小和形状会影响形态学操作的结果.

== 二值图像的膨胀
膨胀操作用于扩展图像中的前景区域 (通常为白色像素). 其基本思想是将结构元素在图像上滑动, 如果结构元素与图像的某个区域有重叠, 则将该区域的中心像素设为前景 (白色).

若原图像为 $A$, 结构元素为 $B$, 则膨胀操作定义为:
$
A plus.circle B = {x | (hat(B))_x union A eq.not emptyset}
$
其中 $hat(B)$ 是结构元素 $B$ 的反射 (i.e. 旋转 $180$ 度), $B_x$ 是结构元素 $B$ 的平移.

用算法流程表示就是
1. 使用反射结构元素扫描图像中的每一个像素
2. 将反射结构元素与其覆盖的二值图像进行逻辑与操作
3. 如果覆盖区域内的运算结果都为 $0$, 则将该像素设为 $0$; 否则设为 $1$.

*膨胀的应用*: 填补图像中的小孔洞, 连接断开的前景区域.

== 二值图像的腐蚀
腐蚀操作用于缩小图像中的前景区域. 其基本思想是将结构元素在图像上滑动, 如果结构元素完全包含在图像的某个区域内, 则将该区域的中心像素设为前景 (白色); 否则设为背景 (黑色).

若原图像为 $A$, 结构元素为 $B$, 则腐蚀操作定义为:
$
A minus.circle B = {x | (B)_x subset.eq A}
$

用算法流程表示就是
1. 使用结构元素扫描图像中的每一个像素
2. 将结构元素与其覆盖的二值图像进行逻辑与操作
3. 如果覆盖区域内的运算结果都为 $1$, 则将该像素设为 $1$; 否则设为 $0$.

== 开运算与闭运算
- *开运算 (Opening)*: 先进行腐蚀操作, 然后进行膨胀操作. 开运算可以去除小的前景噪声, 同时保持较大的前景区域的形状.
$
A circle.small B = (A minus.circle B) plus.circle B
$

- *闭运算 (Closing)*: 先进行膨胀操作, 然后进行腐蚀操作. 闭运算可以填补小的前景孔洞, 同时保持较大的前景区域的形状.
$
A bullet B = (A plus.circle B) minus.circle B
$

= 图像增强
== 基于空域的图像增强
=== 灰度直方图
灰度直方图是图像中各个灰度级别的像素数量的统计. 它可以帮助我们了解图像的对比度、亮度和动态范围. 高对比度的图像通常具有较宽的灰度直方图, 而低对比度的图像则具有较窄的灰度直方图.

=== 直方图均衡化
直方图均衡化是一种增强图像对比度的方法. 其基本思想是通过重新分配图像的灰度级别, 使得灰度直方图更加均匀分布. 直方图均衡化可以增强图像的细节, 特别是在亮度范围较窄的图像中.

直方图均衡化的步骤如下:
1. 统计各灰度级的像素数目 $n_i$, $i = 0, 1, dots, L-1$.
2. 计算原始图像直方图各灰度级的频数 $p_i = n_i / (M times N)$, 其中 $M times N$ 是图像的总像素数.
3. 计算累积分布函数 (CDF): $"CDF"(i) = sum_(j=0)^(i) p_j$, $i = 0, 1, dots, L-1$.
4. 计算映射函数: $s_k = (L - 1) "CDF"(r_k)$, 其中 $r_k$ 是原始图像的灰度级, $s_k$ 是均衡化后的灰度级.
5. 使用映射函数将原始图像的灰度级转换为均衡化后的灰度级.

=== 局部直方图均衡化
前述直方图均衡化是基于全图的, 适用于整体增强, 但当目的是增强图像中几个小区域的细节时, 通常就会失败. 这是因为在这些小区域中, 像素的数量对计算全局变换的影响可以忽略.

局部直方图均衡化 (LHE) 是一种基于局部区域的图像增强方法. 其基本思想是对图像的每个像素, 使用其邻域内的像素来计算局部直方图, 然后应用直方图均衡化. 这样可以增强图像中的局部细节, 特别是在亮度变化较大的区域.

=== 直方图匹配 (规定化)
直方图匹配是一种将图像的灰度分布调整为指定分布的方法. 其基本思想是通过计算原始图像和目标分布的累积分布函数 (CDF), 然后使用这些 CDF 来重新映射图像的灰度级别.

直方图匹配的步骤如下:
1. 计算原始图像和目标图像的灰度直方图和累积分布函数 $"CDF"_("source")(i)$ 和 $"CDF"_("target")(i)$.
2. 对于原始图像的每个灰度级 $r_k$, 找到目标图像中使得 $"CDF"_("target")(s_k) approx "CDF"_("source")(r_k)$ 的灰度级 $s_k$.
3. 使用映射函数将原始图像的灰度级转换为目标图像的灰度级.

=== 空间域滤波器
*线性滤波器*: 通过卷积操作实现, 可以用于平滑 (如均值滤波) 或锐化 (如拉普拉斯滤波)
$
  g(x, y) = sum_(s=-a)^(a) sum_(t=-b)^(b) w(s, t) f(x + s, y + t)
$
这里的 $w(s, t)$ 是滤波器的权重矩阵 (卷积核).

*非线性滤波器*: 不依赖于线性卷积, 可以更有效地处理某些类型的噪声 (如中值滤波)

=== 空间域平滑
*均值滤波器*: 通过计算邻域内像素的平均值来平滑图像, 可以有效地减少高斯噪声, 但会模糊图像细节.
$
g(x, y) = (1)/((2a + 1) (2b + 1)) sum_(s=-a)^(a) sum_(t=-b)^(b) f(x + s, y + t)
$
*超限像素平滑法*: 通过识别和替换异常像素来平滑图像, 可以有效地去除椒盐噪声, 同时保留图像细节.
$
  g'(x, y) = cases(
    g(x, y) "if" |f(x, y) - g(x, y)| < T,
    f(x, y) "otherwise"
  )
$
其中 $f(x, y)$ 是原始图像, $g(x, y)$ 是均值滤波后的图像, $T$ 是阈值.

*中值滤波器*: 通过计算邻域内像素的中值来平滑图像, 可以有效地去除椒盐噪声, 同时保留图像边缘.
$
g(x, y) = "median"{f(x + s, y + t), -a <= s <= a, -b <= t <= b}
$

=== 空间域锐化
图像锐化指的是对图像的边缘或轮廓进行增强, 以达到增强视觉效果的目的. 由于边缘通常出现在灰度突变的地方, 所以锐化的实现通常依赖于图像的梯度信息.

=== 梯度锐化法
在离散图像 $f(x, y)$ 中, 其梯度可以通过以下方式近似计算:
- $G_x = f(x + 1, y) - f(x, y)$
- $G_y = f(x, y + 1) - f(x, y)$
它们可以分别对应两个卷积核:
$
G_x = [1, -1], G_y = [1; -1]^T
$
它们被称为 *梯度算子* (gradient operator). 除梯度算子外, 还有很多其他的算子, 如 Roberts 算子
$
  mat(delim: "[",
  -1, 0 ;
  0, 1), 
  mat(delim: "[",
  0, -1 ;
  1, 0)
$
为了在锐化边缘的同时减少噪声的影响, Prewitt加大了边缘增强算子的模板大小, 其算子为
$
  G_("hori") = mat(delim: "[",
  -1, -1, -1 ;
  0, 0, 0 ;
  1, 1, 1),
  G_("vert") = mat(delim: "[",
  -1, 0, 1 ;
  -1, 0, 1 ;
  -1, 0, 1),
  G_("diag") = mat(delim: "[",
  0, 1, 1 ;
  -1, 0, 1 ;
  -1, -1, 0)
$

== 基于频域的图像增强
图像的低频分量, 对应到图像灰度变化平缓的部分; 图像的高频分量, 对应到灰度变化剧烈的区域, 例如细节噪声和边缘. 因此, 通过滤波器来增强或抑制图像的某些频率成分, 可以达到图像增强的目的.
=== 理想低通滤波器 ILPF
理想低通滤波器 (Ideal Low Pass Filter, ILPF) 在频域中定义为:
$
H(u, v) = cases(
  1 quad D(u, v) <= D_0,
  0 quad D(u, v) > D_0
)
$
其中 $D(u, v) = sqrt((u - u_0)^2 + (v - v_0)^2)$ 是频率点 $(u, v)$ 到频率中心 $(u_0, v_0)$ 的距离, $D_0$ 是截止频率. ILPF 会完全保留低于截止频率的成分, 并完全抑制高于截止频率的成分.

=== Butterworth 低通滤波器 BLPF
Butterworth 低通滤波器 (Butterworth Low Pass Filter, BLPF) 在频域中定义为:
$
H(u, v) = 1 / (1 + (D(u, v) / D_0)^(2n))
$
其中 $n$ 是滤波器的阶数, 控制滤波器的陡峭程度. BLPF 提供了一个平滑的过渡, 相比 ILPF 更少引入振铃效应.

= 图像退化和复原
== 图像退化
=== 图像退化类型
- *规则图案变形*: 由已知的几何变换引起的图像变形, 如旋转、缩放和平移.
- *边缘/运动模糊*: 由于成像系统的点扩散函数 (PSF) 引起的图像模糊, 如运动模糊和散焦模糊.
- *噪声污染*: 由传感器噪声或传输错误引起的图像噪声, 如高斯噪声和椒盐噪声.

=== 图像退化的数学模型
$
g(x, y) = h(x, y) * f(x, y) + n(x, y)
$
在频域中表示为:
$
G(u, v) = H(u, v) F(u, v) + N(u, v)
$
这里的 $g(x, y)$ 是退化后的图像, $f(x, y)$ 是原始图像, $h(x, y)$ 是退化函数, $n(x, y)$ 是噪声.

常见的噪声类型有
- *高斯噪声*: 服从高斯分布的随机噪声, 常见于电子设备产生的噪声.
- *椒盐噪声*: 噪声值随机地取最大值 (灰度值 $=255$, 盐噪声) 或最小值 (灰度值 $=0$, 椒噪声), 常见于传输错误或故障.
- *瑞利噪声*: 服从瑞利分布的随机噪声, 常见于雷达成像等应用.
- *伽马噪声*: 服从伽马分布的随机噪声, 常见于医学成像等应用.
- *均匀噪声*: 在一定范围内均匀分布的随机噪声.

=== LTI 系统近似
在图像退化模型中, 退化函数 $h(x, y)$ 通常被假设为线性时不变系统 (LTI), 或者为一系列 LTI 系统的组合. 这种假设简化了图像复原问题, 使得我们可以使用卷积和频域分析来处理图像退化.
=== 点扩散函数 (PSF)
点扩散函数 (Point Spread Function, PSF) 描述了成像系统对一个点光源的响应. 
$
  h(x, alpha , y, beta) = H dot.c delta(x - alpha, y - beta)
$
其中 $H$ 是系统的增益, $delta(x - alpha, y - beta)$ 是二维单位冲激函数, 表示点光源在位置 $(alpha, beta)$ 处的响应. 在这种情况下, 退化的数学模型可以表示为:
$
g(x, y) = integral.double_( - oo)^( + oo) f(alpha, beta) h(x, alpha , y, beta) dif alpha dif beta + n(x, y)
$
=== 离散系统的退化模型
在离散图像处理中, 退化模型可以表示为:
$
g(m, n) = sum_(s=-a)^(a) sum_(t=-b)^(b) h(s, t) f(m - s, n - t) + n(m, n)
$
这里的 $g(m, n)$ 是退化后的图像, $f(m, n)$ 是原始图像, $h(s, t)$ 是离散的退化函数, $n(m, n)$ 是离散的噪声. 它可以被写作矩阵形式:
$
[g] = [H][f] + [n]
$
展开来就是
$
mat(delim: "[",
g(0); g(1); dots.v ; g(M N-1)
) = mat(delim: "[",
H_0, H_(M-1), dots.h , H_1;
H_1, H_0, dots.h , H_2;
dots.v, dots.v , dots.down , dots.v ;
H_(M-1), H_(M-2), dots.h , H_0
) times mat(delim: "[",
f(0); f(1); dots.v ; f(M N-1) 
) + mat(delim: "[",
n(0); n(1); dots.v ; n(M N-1)
) 
$
其中 $H_i$ 被定义为
$
H_i = mat(delim: "[",
h_e (i, 0), h_e (i, N-1), dots.h , h_e (i, 1);
h_e (i, 1), h_e (i, 0), dots.h , h_e (i, 2);
dots.v, dots.v , dots.down , dots.v ;
h_e (i, N-1), h_e (i, N-2), dots.h , h_e (i, 0)
)
$

== 图像复原
图像复原是指通过已知的退化模型和噪声统计特性, 来估计原始图像. 

#showybox(
  title: "图像复原和图像增强的区别",
  frame: frameSettings,
  // footer: "Information extracted from a well-known public encyclopedia"
)[
  *图像增强* 不考虑图像降质的原因, 只将图像中感兴趣的特征有选择地突出, 从而衰减不需要的特征. 改善后的图像不一定要去逼近原图像; *图像复原* 则是试图去逆转已知的图像降质过程, 以恢复原始图像. 图像复原通常需要对降质过程有一个明确的数学模型, 要建立评价复原好坏的客观标准.

  简而言之, 图像增强侧重于改善图像的视觉效果, 而图像复原侧重于恢复初始图像. 对一幅已经退化的图像, 通常的做法是先做图像复原, 然后再做图像增强.
]

== 无约束图像复原
=== 代数复原方法
假设图像退化模型为
$
  n = g - H f
$
我们希望找到一个 $f$ 的估计, 使得误差 $n$ 最小. 这可以通过最小化以下目标函数来实现:
$
J(f) = ||g - H f||^2
$
此处的 $J$ 不依赖于任何先验知识, 因此称为无约束复原方法. 通过对 $J(f)$ 求导并设导数为零, 可以得到最小化 $J(f)$ 的解:
$
f^ = (H^T H)^(-1) H^T g = H^(-1) g
$
=== 逆滤波
逆滤波是一种基于频域的图像复原方法. 假设图像退化模型为
$
G(u, v) = H(u, v) F(u, v) + N(u, v)
$
逆滤波的目标是通过除以退化函数 $H(u, v)$ 来恢复原始图像 $F(u, v)$:
$
F^(u, v) = (1)/(H(u, v)) [G(u, v) - N(u, v)]
$

然而, 逆滤波对噪声非常敏感, 特别是在 $H(u, v)$ 接近零的频率处. 因此, 逆滤波通常只适用于噪声较小的情况.

== 有约束图像复原
无约束复原方法中的逆滤波虽然比较简单, 但并没有说明如何处理噪声. 而有约束复原方法则考虑了噪声的影响, 并通过引入先验知识来改善复原效果.

=== 代数有约束复原方法
在有约束复原方法中, 我们希望计算函数 $||Q f||$ 在约束条件 $||g - H f||^2 = ||n||^2$ 下的最小值, 其中 $Q$ 是一个正则化矩阵, 用于引入先验知识. 通过拉格朗日乘数法, 可以将其转化为无约束优化问题:
$
J(f) = ||Q f||^2 + lambda (||g - H f||^2 - ||n||^2)
$
可以通过对 $J(f)$ 求导并设导数为零, 得到最小化 $J(f)$ 的解:
$
f^ = (H^T H + (1)/(lambda) Q^T Q)^(-1) H^T g
$

我们也可以设置别的约束条件, 如 $||g||^2 = ||f||^2$.

= 图像压缩
== 图像压缩背景
=== 图像压缩合理性
- 一般原始图像中存在很大的冗余度
- 用户通常允许图像失真
- 当信道的分辨率不及原始图像的分辨率时, 降低输入的原始图像的分辨率对输出图像分辨率影响不大
- 用户对原始图像的信号不全都感兴趣, 可用特征提取和图像识别的方法, 丢掉大量无用的信息, 提取有用的信息.
=== 图像冗余
*编码冗余*
如果一个图像的灰度级编码, 使用的比特数超过了该图像实际所需的最少比特数, 则称该图像存在编码冗余. 例如, 使用 $8$ 比特来表示灰度级别的图像, 但实际上只使用了 $100$ 个灰度级别, 则存在编码冗余.

*像素冗余*
对于一幅图像, 很多单个像素对视觉的贡献是冗余的. 原始图像越有规则, 各像素之间的相关性越强, 像素冗余就越大. 例如, 在一幅平坦区域的图像中, 相邻像素的灰度值通常非常接近, 因此存在较大的像素冗余.

*心理视觉冗余*
人眼对不同频率和颜色的敏感度不同. 例如, 人眼对高频细节和某些颜色变化不敏感, 因此可以通过去除这些不敏感的信息来实现图像压缩, 这就是心理视觉冗余.

=== 无损压缩和有损压缩
无损压缩是指在压缩过程中不丢失任何信息, 压缩后的图像可以完全恢复为原始图像. 常见的无损压缩方法格式有: PNG, GIF, BMP 等.

有损压缩是指在压缩过程中丢失部分信息, 压缩后的图像无法完全恢复为原始图像, 但通常可以达到视觉上不可察觉的效果. 常见的有损压缩方法格式有: JPEG, WebP 等.

=== 图像压缩的保真度准则
图像信号在编码和传输过程中会产生误差, 尤其是在有损压缩编码中, 产生的误差应在允许的范围之内. 在这种情况下, 保真度准则可以用来衡量编码方法或系统质量的优劣.

*客观保真度准则*
- 均方根误差 (RMSE): 衡量原始图像和压缩图像之间的平均误差 $ "RMSE" = sqrt((1)/(M N) sum_(x=0)^(M-1) sum_(y=0)^(N-1) [f(x, y) - hat(f) (x, y)]^2) $
- 均方信噪比 (SNR): 衡量信号强度与噪声强度的比值 $ "SNR" = ((sum_(x=0)^(M-1) sum_(y=0)^(N-1) [f(x, y)]^2) / (sum_(x=0)^(M-1) sum_(y=0)^(N-1) [f(x, y) - hat(f) (x, y)]^2)) $ 信噪比越大, 表示压缩图像质量越好.

*主观保真度准则*
通过人眼对图像的主观评价来衡量图像质量, 例如对比度、清晰度和色彩还原度等.
== 无损压缩方法
=== 压缩率
压缩率 (Compression Ratio, CR) 是衡量图像压缩效果的一个重要指标, 定义为原始图像大小与压缩后图像大小之比:
$
"CR" = ("Size"_("original")) / ("Size"_("compressed"))
$
无损压缩的压缩率一般较低, 通常在 $2:1$ 到 $10:1$ 之间, 具体取决于图像的内容和所使用的压缩算法.

=== 变长编码: Huffman 编码
Huffman 编码是一种基于字符出现频率的无损压缩方法. 它通过构建一棵二叉树, 将频率较高的字符分配较短的编码, 频率较低的字符分配较长的编码, 从而实现压缩. Huffman 编码的步骤如下:
1. 统计图像中各灰度级的出现频率.
2. 根据频率构建 Huffman 树.
3. 为每个灰度级分配二进制编码.
4. 使用分配的编码对图像进行编码.

Huffman 编码是一种 *最优变长编码*, 在已知符号概率分布的情况下, 可以实现最小的平均编码长度; 也是一种 *快码*, 因为各个信源符号都被解成一组固定次序的码符号; 同时也是一种 *前缀码 (即时码)*, 即任何一个码字都不是另一个码字的前缀, 这样可以确保编码的唯一可解析性, 从而提升解码效率.

=== 变长编码: 算数编码
算数编码是一种基于概率模型的无损压缩方法. 它通过将整个消息映射到一个实数区间 [0, 1) 上, 并根据符号的概率分布不断缩小该区间, 最终生成一个实数作为编码结果. 算数编码的步骤如下:
1. 统计各字符的出现频率, 并计算其概率分布.
2. 按照各字符的概率分布, 将区间 [0, 1) 划分为若干子区间. 比如说, 有三个符号 A, B, C, 其概率分别为 0.5, 0.3, 0.2, 则可以划分为 [0, 0.5), [0.5, 0.8), [0.8, 1).
3. 对于输入消息, 逐个符号处理, 不断缩小区间. 例如, 对于消息 "AB", 首先处理 A, 将区间缩小到 [0, 0.5); 然后处理 B, 将区间缩小到 [0.25, 0.4).
4. 最终选择区间内的任意一个二进制表示最短的实数作为编码结果.

=== LZW编码
LZW (Lempel-Ziv-Welch) 编码是一种基于字典的无损压缩方法. 它通过动态构建一个字典, 将输入数据中的重复模式替换为较短的代码, 从而实现压缩. 在编码处理的开始阶段, LZW 编码器初始化一个包含所有可能单字符模式的字典. 对于8位单色图像, 字典中前256个字被分配给灰度值0 \~ 255. 当编码器顺序地分析图像像素的时候, 字典中没有包括的灰度级序列由算法决定其出现的位置

== 有损压缩方法
=== 变换编码
将原来在空域描述的图像信号, 通过某种数学变换转换到另外一些正交空间中去, 用变换系数来表示图像信号, 这种方法称为变换编码. 图像变换会使图像信号能量在空间重新分布, 其中低频成分占据能量绝大部分, 而高频成分只占有很小一部分能量. 

一般来说在变换域里描述比在空域简单, 因为图像相关性明显下降. 尽管变换本身并不带来数据压缩, 但是但由于变换图像的能量大部分只集中于少数几个变换系数上, 因此可以通过对变换系数进行量化和编码来实现图像压缩.

*DFT 编码*: 离散傅里叶变换 (Discrete Fourier Transform, DFT) 将图像从空间域转换到频域. DFT 编码通过保留低频成分并丢弃高频成分来实现压缩. 然而, DFT 编码存在计算复杂度高和阻塞效应明显的问题.

*DCT 编码*: 离散余弦变换 (Discrete Cosine Transform, DCT) 是一种常用的图像变换方法. DCT 编码通过将图像划分为小块 (通常为 $8 times 8$), 对每个块进行 DCT 变换
$
C(u, v) = (1)/(4) alpha(u) alpha(v) sum_(x=0)^(7) sum_(y=0)^(7) f(x, y) cos[(2x + 1) u pi / 16] cos[(2y + 1) v pi / 16]
$ 
然后对变换系数进行量化和编码来实现压缩. DCT 编码具有较低的计算复杂度和较好的压缩性能, 广泛应用于 JPEG 图像压缩标准中.

= 图像物体表达
== 面向识别任务的物体表达
- *传统识别任务*: ImageNet 图像分类, 如 AlexNet, VGG, ResNet 等卷积神经网络 (CNN) 架构.
- *人脸识别任务*: DeepID, VS2VI
- *小样本/零样本识别任务*: 给定少量 (或没有, 比如说通过训练几个品种的鸟类数据, 让模型能识别去其他种类的鸟) 的有标注数据, 学习一个可以识别这些类别数据的模型. 
=== 零样本识别方法类型
- 基于表征学习的零样本学习方法: 直接从图像中学习特征表示, 然后将这些特征映射到语义空间中进行分类.
- 基于生成模型的零样本学习方法: 使用生成模型 (如生成对抗网络, GAN), 先用可见类别的数据训练生成模型, 然后生成不可见类别的样本, 最后使用这些生成的样本进行分类器训练.

=== GAN
生成对抗网络 (Generative Adversarial Network, GAN) 由两个神经网络组成: 生成器 (Generator) 和判别器 (Discriminator). 生成器负责生成逼真的图像, 判别器负责区分真实图像和生成图像. 通过对抗训练, 生成器不断改进其生成能力, 最终能够生成高质量的图像.

GAN 的训练过程可以表示为一个最小化最大化的问题:
$
min_G max_D V(D, G) = E_(x ~ p_"data" (x)) [log D(x)] \
+ E_(z ~ p_z(z)) [log(1 - D(G(z)))]
$
其中, $G$ 是生成器, $D$ 是判别器, $p_"data"(x)$ 是真实数据的分布, $p_z(z)$ 是噪声的分布. 对于判别器 $D$ 而言, 它的目标是最大化正确分类真实图像和生成图像的概率; 对于生成器 $G$ 而言, 它的目标是最小化判别器对生成图像的分类错误率.

*CGAN (Conditional GAN)*: 在生成器和判别器中引入条件变量 (如类别标签), 使得生成的图像能够符合特定的条件.
$
min_G max_D V(D, G) = E_(x ~ p_"data" (x)) [log D(x|y)] \
+ E_(z ~ p_z(z)) [log(1 - D(G(z|y)))]
$

=== 开集识别任务
开集识别 (Open Set Recognition, OSR) 任务旨在识别训练集中未见过的类别. 传统的闭集识别方法假设测试数据的类别与训练数据的类别完全相同, 而开集识别方法则允许测试数据包含未知类别.

*判别式方法*: 直接训练一个具有良好泛化性能的开集识别神经网络, 将原有的 $k$ 分类问题转化为 $k + 1$ 分类问题, 其中第 $k + 1$ 类表示未知类别. 这可以通过引入一个拒绝选项来实现, 当模型对某个输入的预测概率低于某个阈值时, 将其归类为未知类别.

*生成式方法*: 对已知类别样本的分布进行建模, 根据测试样本是否符合建模的分布. 如果不符合, 则将其归类为未知类别. 这可以通过使用生成对抗网络 (GAN) 或变分自编码器 (VAE) 来实现, 以学习已知类别的特征分布.
== 神经场景表达
=== NeRF
神经辐射场 (Neural Radiance Fields, NeRF) 是一种用于表示和渲染复杂 3D 场景的神经网络方法. NeRF 使用一个多层感知机 (MLP) 来表示场景中的体积密度和颜色信息. 给定一个 3D 坐标 $(x, y, z)$ 和一个视角方向 $(theta, phi)$, NeRF 输出该位置的颜色 $c$ 和体积密度 $sigma$:
$
(c, sigma) = "MLP"(x, y, z, theta, phi)
$
通过对场景进行采样和体积渲染, NeRF 可以生成高质量的视图. 具体来说, 对于每个像素, NeRF 沿着视线采样多个点, 计算每个点的颜色和密度, 然后使用体积渲染公式将这些信息合成最终的像素颜色.

= 运动检测
== 背景差法
计算当前图像与预设背景图像的逐像素的灰度差, 通过设置阈值来确定运动前景区域. 那么我们如何建立背景图像呢? 
=== 均值图像
对于静止的背景, 可以通过对一段时间内的图像序列进行逐像素的均值计算来建立背景图像. 具体来说, 对于每个像素位置 $(x, y)$, 计算该位置在 $N$ 帧图像中的平均灰度值:
$
B(x, y) = (1)/(N) sum_(i=1)^(N) I_i (x, y)
$
其中, $I_i (x, y)$ 是第 $i$ 帧图像在位置 $(x, y)$ 的灰度值, $B(x, y)$ 是背景图像在该位置的灰度值.
=== 单高斯模型
对于背景不是绝对静止的场景, 如树枝摇曳或窗帘晃动, 可以使用单高斯模型, 即用一个高斯分布来描述每个像素在不同时刻的灰度分布情况:
$
P(I(x, y)) = (1)/sqrt(2 pi sigma^2 (x, y)) exp(- (I(x, y) - mu(x, y))^2 / (2 sigma^2 (x, y)))
$
=== 混合高斯模型
混合高斯模型 (Mixture of Gaussians, MoG) 使用多个高斯分布来描述每个像素的灰度分布, 以更好地适应复杂的背景变化. 对于每个像素位置 $(x, y)$, 混合高斯模型可以表示为:
$
P(I(x, y)) = sum_(k=1)^(K) w_k (x, y) cal(N)  (mu_k (x, y), sigma_k^2 (x, y))
$

*混合高斯模型的背景建模算法*
1. 在模型开始时, 使用第一幅图像该点的像素值作为均值 $mu$, 并给定一个较大的方差 $sigma^2$ 和较小的权重 $w$.
2. 在后续图像序列中, 模型对每个像素点进行迭代更新. 如果当前的像素值与某个高斯分布的均值接近 (通常定义为在 $2.5 sigma$ 范围内), 则认为该像素值属于该高斯分布, 并更新该分布的参数:
  - 更新均值:
  $
    mu_k (x, y) = (1 - alpha) mu_k (x, y) + alpha I(x, y)
  $
  - 更新方差:
  $
    sigma_k^2 (x, y) = (1 - alpha) sigma_k^2 (x, y) + alpha (I(x, y) - mu_k (x, y))^2
  $
  - 更新权重:
  $
    w_k (x, y) = (1 - alpha) w_k (x, y) + alpha
  $
3. 如果当前的像素值不属于任何一个高斯分布, 且当前的高斯分布数量未达到预设的最大值 $K$, 则创建一个新的高斯分布, 其均值为当前像素值, 方差为较大的初始值, 权重为较小的初始值.
4. 确定背景模型: MoG 模型假设 *背景在图像序列中总是最经常被观测到*, 通过定义各个高斯分布的优先级 $w / sigma$ 来选择前 $B$ 个高斯分布作为背景模型, 其中 $B$ 满足: $sum_(k=1)^(B) w_k > T$, $T$ 是一个预设的阈值 (通常取 $0.7$ 到 $0.9$ 之间).

== 光流法
光流是空间运动物体在观测成像面上像素运动的瞬时速度. 光流的研究利用图像序列中像素强度数据的时域变化和相关性来确定各自像素位置的 "运动".

=== 光流约束方程
光流的计算基于 *光流一致性假设*: 在短时间内, 图像中某一点的亮度值保持不变. 设图像亮度函数为 $I(x, y, t)$, 则有:
$
I(x, y, t) = I(x + delta x, y + delta y, t + delta t)
$
写成微分形式, 并忽略高阶小量, 得到光流约束方程:
$
(partial I) / (partial x) u + (partial I) / (partial y) v + (partial I) / (partial t) = 0
$

=== Lucas-Kanade 方法
光流约束方程有一个问题, 就是它只有一个方程, 但有两个未知数 $u$ 和 $v$, 因此无法直接求解. 这就是 *孔径问题 (Aperture Problem)*, 你无法仅凭一个像素点的信息确定它的完整运动, 就像通过一个小孔观察一个移动的边缘, 你只能感知到垂直于边缘的运动.

Lucas-Kanade 方法引入了 *局部运动一致性*. 它不只看单个像素, 而是考察该像素周围的一个小窗口 (例如 $5 times 5$ 像素). LK 方法假设这个窗口内的所有像素共享同一个运动向量. 

这样一来, 问题就改变了: 原来是一个方程解两个未知数, 现在是窗口内的所有像素 (比如 25 个) 共同来解这两个未知数. 这就形成了一个超定方程组. LK 方法使用最小二乘法找到这个方程组的最优解, 这个解就是该窗口的整体运动向量.

== 帧间差分法
计算相邻两帧 (或多帧) 图像的逐像素灰度差, 并设置阈值来确定运动前景区域. 具体来说, 对于 $N$ 帧图像 $I_t (x, y), I_(t-1) (x, y), ..., I_(t-N+1) (x, y)$, 计算当前帧与前 $N-1$ 帧的差分:
$
D_t (x, y) = product_(i=1)^(N-1) |I_(t - i + 1) (x, y) - I_(t - i) (x, y)|
$
通过设置阈值 $T$, 可确定运动前景区域:
$
M_t (x, y) = cases(
  I_t (x, y) quad D_t (x, y) >= T,
  0 quad "otherwise"
)
$

= 目标检测
== 滑窗法
1. *生成备选检测结果 (滑窗与分类)*
  - 在图像上以不同的尺度和位置滑动一个固定大小的窗口, 对每个窗口内的图像区域进行分类
  - 将分类概率较高的窗口保留下来, 作为备选检测结果

2. *非极大值抑制 (NMS)*
  - 对备选检测结果进行排序, 按照分类概率从高到低
  - 依次选择排序靠前的检测结果, 并将与其重叠度 (IoU, inter over union) 较高的其他检测结果删除 $ "IoU" = (|A inter B|)/(|A union B|) $
  - 重复上述过程, 直到所有检测结果都被处理完毕

*AdaBoost 分类器*
AdaBoost (Adaptive Boosting) 是一种集成学习方法, 融合多个弱分类器形成一个整体的强分类器. 它通过给样本赋予权重, 将注意力更多地放在 "难分" 的样本上, 并将准确率越高的弱分类器赋予越高的权重. 为了提高检测速度和精度, 最终的分类器通常通过多个强分类器级联 (Cascade) 来实现.

== 两阶段检测器
1. *生成候选区域 (Region Proposal)*
  - 使用区域建议网络 (Region Proposal Network, RPN) 或选择性搜索 (Selective Search) 等方法, 在图像中生成一组可能包含目标的候选区域 (Region of Interest, RoI)
2. *分类与边界框回归 (Bounding Box Regression)*
  - 对每个候选区域进行特征提取, 通常使用卷积神经网络 (CNN)
  - 使用全连接层对提取的特征进行分类, 并预测边界框的精确位置

例子: R-CNN, Fast R-CNN

== 一阶段检测器
将目标检测框架设计为回归问题, 直接从图像像素预测包围框以及概率.

*YOLO (You Only Look Once)*
1. 将输入图像划分为 $S times S$ 的网格单元
2. 每一个网格单元预测 $B$ 个边界框, 每个边界框的置信度以及 $C$ 个类别的概率. 所以整个网络的输出为 $S times S times (5 B + C)$
3. 最后再通过非极大值抑制 (NMS) 来过滤掉重叠的边界框和低置信度的检测结果
= 图像分割
== 图像分割的主要类别
- *基本图像分割*: 将图像划分为若干个区域, 每个区域内的像素具有相似的属性 (如颜色、纹理等)
- *语义分割 (Semantic Segmentation)*: 将图像中的每个像素分配给一个预定义的类别, 使得同一类别的像素具有相同的标签
- *实例分割 (Instance Segmentation)*: 将图像中相同类别目标的不同实例分别分割出来
- *全景分割 (Panoptic Segmentation)*: 结合语义分割和实例分割, 对图像中的每个像素进行分类, 同时区分不同实例.
== 阈值法 (Thresholding)
基本原理是通过设定不同的特征阈值, 把图像像素点分为若干类. 数学上, 若 $I(x, y) > T$, 则像素点 $(x, y)$ 属于前景; 否则属于背景. 其中 $T$ 是预设的阈值.

=== 全局阈值方法
- *$p$-分位法*: 根据图像灰度直方图, 选择使得前景和背景像素数目比例为 $p:(1-p)$ 的阈值 $T$. 这里 $p$ 是一个预设的先验概率.
- *众数法*: 选择图像灰度直方图中的众数 (出现频率最高的灰度值) 作为阈值 $T$.
- *迭代法*: 初始阈值选取为图像的平均灰度 $T_0$, 然后用 $T_0$ 将图像的像素点分作两部分. 计算每部分各自的平均灰度, 小于 $T_0$ 的部分平均灰度为 $mu_1$, 大于 $T_0$ 的部分平均灰度为 $mu_2$. 然后更新阈值为 $T_1 = (mu_1 + mu_2) / 2$. 重复上述过程, 直到阈值收敛.

=== 局部阈值方法
基本原理是将图像分块, 分别用全局阈值方法分割, 然后将各块的分割结果合并. 适用于光照不均匀的图像.

== 区域生长法 (Region Growing)
区域生长法是一种基于像素相似性的图像分割方法. 其基本思想是从一个或多个种子点开始, 将与种子点相似的邻域像素逐步加入到区域中, 直到没有更多的像素满足相似性条件为止.

1. *选择种子点*: 根据某种准则 (如手动选择或自动检测) 选择一个或多个种子点作为初始区域.
2. *定义相似性准则*: 设定一个相似性度量 (如灰度差异、颜色距离等), 用于判断邻域像素是否与当前区域相似.
3. *区域生长*: 对每个种子点, 检查其邻域像素, 如果某个邻域像素满足相似性准则, 则将其加入到区域中, 并将该像素作为新的种子点继续生长. 重复此过程, 直到没有更多的像素满足相似性条件为止.
4. *合并区域*: 如果有多个种子点, 可以根据区域间的相似性将相邻的区域合并.

== 分水岭分割法 (Watershed)
想象图像是一个地形表面: 高度值对应于像素的灰度值, 暗像素对应于低洼区域, 亮像素对应于高地. 

算法的本质就是模拟水面上升的过程. 在每一个低谷 (图像中的暗区, 通常是物体内部) 都打了一个洞, 让水慢慢从最低点开始灌入. 随着水位的上升, 水会逐渐填满低洼区域. 当两个不同的水域即将相遇时, 就在它们之间建立一道堤坝 (分水岭线), 以防止它们混合在一起. 

最终, 当水面完全淹没整个地形时, 这些堤坝就形成了图像的分割边界:
- 堤坝内部 (集水盆): 它们是连续、同质的区域, 代表了图像中的一个独立物体.
- 堤坝线 (分水岭): 它们是不同区域之间的边界, 代表了物体之间的分割线.

== 均值移动 (Mean Shift)
让每一个数据点 (像素) 向它周围最密集的区域中心移动, 直到稳定下来.

在 Mean Shift 的语境下, 我们首先要将图像中的每一个像素点看作是多维空间中的一个 "数据点". 这个多维空间通常包含两个主要部分, 空间坐标 $(x, y)$ 和颜色信息 (如 RGB 值). 因此, 每个像素点可以表示为一个五维向量 $(x, y, R, G, B)$.

*移动过程*
1. 定义邻域 (窗): 对于每个像素点, 设定一个固定大小的窗口 (或者更准确地说是核函数), 包含该点及其周围的像素点. 这个窗口定义了你只关心哪些邻近的像素点.
2. 计算均值 (重心): 在这个窗口内, 计算所有数据点的加权平均值 (即重心). 这个重心就是窗口内数据最密集的方向, 也就是密度增加最快的方向.
3. 然后, 将起始点移动到重心位置. 重复上述过程, 直到点的位置不再发生显著变化 (即收敛).

当所有像素点都执行完这个 "寻找密度中心" 的过程后, 分割就自然完成了. 所有初始位置不同的像素点, 如果最终收敛到了同一个密度峰值点, 那么它们就被视为同一类, 即它们属于图像中的同一个区域或物体.

== 图割 (Graph Cut)
图割是一种将图像分割问题转化为图论中最小割问题的优化方法. 将图像表示为图 $G = (V, E)$, 其中顶点集 $V$ 包含图像中的像素点和两个特殊节点: 源节点 $s$ 和汇节点 $t$. 边集 $E$ 包含连接像素点之间的边以及连接像素点与源节点和汇节点的边:
$
  V= {p_1, p_2, ..., p_n} union {s, t} \
  P = {(p_i, p_j) | "if" p_i "and" p_j "are neighbors"} union {(s, p_i) } union {(p_i, t)}
$

一个像素节点连接到 $s$ 的边的权重, 代表了这个像素属于前景的 "可能性". 同理, 它连接到 $t$ 的边的权重, 代表了它属于背景的 "可能性". 这种 "可能性" 通常来自用户的先验指定, 例如用户在图像上用画笔大致标出几个属于前景和背景的区域.

图构建完成后, 分割的过程就是在这个图上寻找一个 "切割" (Cut). 一个 "切割" 指的是移除图中的一些边, 使得源点 $s$ 和 汇点 $t$ 被完全分离, 不再有任何路径可以连通. 切割会移除多条边, 每条边都有自己的权重 (代价). 图割算法的目标, 是寻找一种切割方案, 使得被移除的所有边的权重总和最小. 这个最终的方案, 就是 *"最小割" (Minimum Cut)*
== 全卷积网络 (Fully Convolutional Networks, FCN)
== DeepLab
= 特征提取
== 边缘检测
边缘被定义为图像中亮度突然变化的区域, 是图像灰度构成的曲面上的陡峭区域, 或像素灰度存在阶跃变化或屋脊状变化的像素集合.

图像边缘提取通常包含三个主要步骤
1. *抑制噪声*: 通过低通滤波, 平滑, 去噪, 模糊等手段
2. *边缘特征增强*: 通过高通滤波、锐化来增强边缘特征
3. *边缘定位*: 提取边缘常使用微分滤波器

=== 一阶微分算子 
如 Prewitt 算子和 Sobel 算子, 通过检测一阶导数的极大值点来识别边缘. Sobel 和 Prewitt 算子在近似一阶微分的同时, 也加入了计算均值和平滑噪声的功能.

=== LoG 算子
Laplacian of Gaussian (LoG) 算子结合了高斯平滑和拉普拉斯算子, 先对图像进行高斯平滑, 然后应用拉普拉斯算子检测边缘.
$
"LoG"(x, y) = - (1)/(pi sigma^4) [1 - (x^2 + y^2)/(2 sigma^2)] exp[- (x^2 + y^2)/(2 sigma^2)]
$
其中, $[1 - (x^2 + y^2)/(2 sigma^2)]$ 是高斯函数求导两次的结果, $exp[- (x^2 + y^2)/(2 sigma^2)]$ 是高斯平滑项.

=== Canny 边缘检测器
1. *噪声抑制*: 使用高斯滤波器对图像进行平滑处理, 减少噪声对边缘检测的影响.
2. *计算梯度强度和方向*: 通过 Sobel 算子计算图像中每个像素的梯度强度和方向.
3. *非极大值抑制*: 这一步的目标是细化边缘, 将粗边缘变成细边缘. 在梯度强度图中, 边缘通常有几个像素宽度. 非极大值抑制通过扫描梯度强度图, 并只保留沿梯度方向上局部最大值的像素点. 对于每个像素点, 算法会比较它的梯度强度与其沿着梯度方向的两个邻居的强度. 如果这个像素点的强度是局部最大值, 则保留它; 否则, 将其强度设置为零, 从而消除了不是最清晰的边缘像素.
4. *双阈值处理*: 使用两个阈值 (高阈值和低阈值) 来区分强边缘和弱边缘. 强边缘是指梯度强度高于高阈值的像素, 弱边缘是指梯度强度介于高阈值和低阈值之间的像素. 对于低于低阈值的像素, 直接将其设为非边缘 (即强度为零).
5. *边缘跟踪*: 最后一步使用滞后阈值的方法将弱边缘连接到强边缘上. 算法从强边缘像素开始, 通过检查其八个邻居, 如果邻居是弱边缘像素, 则认为这个弱边缘像素属于真实边缘, 并将其提升为强边缘. 这个过程会持续传播，直到所有与强边缘相连的弱边缘都被包含进来. 这样, 强边缘得以保留, 与强边缘相连的弱边缘也得以保留, 而那些孤立的、未连接到任何强边缘的弱边缘 (很可能是噪声) 则被最终抛弃.

== 角点检测
角点是图像结构中的关键特征点, 通常指的是两条或多条边缘的交点. 它具有以下核心特性:
- 在图像的 *任何方向* 上移动一个小的 "窗口" 时, 窗口内的像素灰度变化 (梯度) 都很大.
- 它不像平面 (灰度变化很小) 或边缘 (只有一个方向灰度变化大) 那样模糊, 角点提供了最多的信息，因为它限制了物体在二维空间中的运动.

=== Harris 角点检测
Harris 检测器的核心是量化这种 "在所有方向上移动窗口时灰度变化都很大" 的特性. 它通过构造一个自相关矩阵 (或称为结构张量 $M$) 来实现这一点.

对于图像中的一个像素点 $(x, y)$, 我们考虑以它为中心的窗口 $W$. 如果我们将这个窗口沿着一个偏移量 $(Delta x, Delta y)$ 移动, 灰度变化 $E(Delta x, Delta y)$ 可以表示为
$
  E(Delta x, Delta y) = sum_(x, y in W) w(x, y) [I(x + Delta x, y + Delta y) - I(x, y)]^2
$
其中 $I$ 是图像的灰度值, $w(x, y)$ 是窗口内的权重函数 (通常是高斯函数, 它让窗口中心的像素点权重更大).

当偏移量较小时, 可以使用泰勒展开对 $I(x + Delta x, y + Delta y)$ 进行近似:
$
I(x + Delta x, y + Delta y) approx I(x, y) + (partial I)/(partial x) Delta x + (partial I)/(partial y) Delta y
$
对应的灰度变化可以近似为
$
E(Delta x, Delta y) approx [Delta x, Delta y] M [Delta x,  Delta y]^T
$
其中 $M$ 就是自相关矩阵:
$
M = sum_(x, y in W) w(x, y) mat(
  delim: "[",
  I_x^2, I_x I_y;
  I_x I_y, I_y^2
), quad I_x = (partial I)/(partial x), quad I_y = (partial I)/(partial y)
$

矩阵 $M$ 是一个 $2 times 2$ 的对称矩阵, 它的特征值 $lambda_1$ 和 $lambda_2$ 描述了窗口内灰度变化的性质:
- 如果 $lambda_1$ 和 $lambda_2$ 都很大, 则表示窗口内在所有方向上都有显著的灰度变化, 该点是一个角点.
- 如果一个特征值很大, 另一个很小, 则表示窗口内在一个方向上有显著变化, 该点是一个边缘点.
- 如果两个特征值都很小, 则表示窗口内灰度变化不显著, 该点是一个平坦区域.

Harris 本人避免了复杂的特征值计算, 提出了一个角点响应函数 $R$:
$
R = det(M) - k (tr(M))^2 = lambda_1 lambda_2 - k (lambda_1 + lambda_2)^2
$
- 当 $R$ 很大且为正时, 该点是一个角点.
- 当 $R$ 是很小的负值时, 该点是一个边缘点.
- 当 $R$ 接近于零时, 该点是一个平坦区域

== SIFT 特征点检测
SIFT (Scale-Invariant Feature Transform) 是一种用于检测和描述局部特征点的算法, 它寻找哪些在缩放, 旋转和光照变化下仍然稳定存在的关键点.

SIFT 先定义了 *尺度空间* 的概念, 通过对图像进行不同尺度的高斯模糊, 构建一系列模糊图像. 然后, 通过计算相邻尺度图像之间的差异 (DoG, Difference of Gaussian), 来检测潜在的关键点位置. 具体步骤如下:

1. *构建尺度空间*: 对图像进行多次高斯模糊, 生成一系列不同尺度的图像.
2. *计算差分图像*: 计算相邻尺度图像之间的差异, 得到 DoG 图像.
3. *关键点检测*: 在 DoG 图像中寻找局部极大值和极小值, 这些点即为潜在的关键点.
4. *关键点定位*: 对潜在关键点进行精确定位, 并剔除不稳定的点, 包括
  - 低对比度点: 通过计算关键点的对比度, 剔除那些对比度低于某个阈值的点.
  - 边缘响应点: 通过分析关键点的主曲率, 剔除那些位于边缘上的点, 因为它们对噪声较为敏感.
5. *方向分配*: 为每个关键点分配一个主方向, 使得描述符具有旋转不变性. 通过计算关键点邻域内的梯度方向直方图, 选择峰值作为主方向.
6. *关键点描述符*: 在关键点的邻域内, 计算梯度方向和幅值, 并将其组织成一个描述符向量. 通常使用 $16 times 16$ 的邻域, 将其划分为 $4 times 4$ 的子区域, 每个子区域计算 $8$ 个方向的梯度直方图, 最终得到一个 $128$ 维的描述符向量.

== 特征匹配的发展
=== 互相关 (Cross-correlation)
在深度学习和SIFT出现之前, 我们要判断两张图中的两个点是不是同一个物体, 最直观的方法就是 "像素对像素" 的比对. 具体来说, 我们可以在第一张图中选取一个小的窗口 (patch), 然后在第二张图中滑动这个窗口, 计算每个位置上窗口内像素值的相似度. 这种相似度通常通过 *互相关 (Cross-correlation)* 来衡量:
$
  "Score"(m_1, m_2) = (sum sum [I - mu_(m_1)] [J - mu_(m_2)]) / (sqrt(sum sum [I - mu_(m_1)]^2) sqrt(sum sum [J - mu_(m_2)]^2))
$
这个可以被理解为在计算两个去均值后的图像块的的余弦相似度. 互相关的值范围在 $-1$ 到 $1$ 之间, 值越大表示两个图像块越相似.

互相相关方法简单直观, 但是其计算量较大, 且对光照变化和噪声较为敏感.

=== SIFT 特征匹配
正如前文所述, SIFT 算法不仅检测关键点, 还为每个关键点生成一个描述符. 这些描述符是高维向量, 捕捉了关键点邻域内的局部图像结构信息. 但是我们怎么判断两个 SIFT 向量匹配上了呢? 单纯看距离最近是不够的, SIFT 采用 NNDR (Nearest Neighbor Distance Ratio):
$
  "NNDR" = (d(m, m_"best")) / (d(m, m_"second best"))
$
其中 $d(m, m_"best")$ 是当前描述符 $m$ 与其最近邻描述符 $m_"best"$ 之间的距离, $d(m, m_"second best")$ 是与次近邻描述符 $m_"second best"$ 之间的距离. 如果 NNDR 小于某个阈值 (通常取 $0.8$), 则认为这两个描述符匹配成功.

=== L2-Net
L2-Net 是一种基于深度学习的局部特征描述符, 旨在生成具有更强区分能力和鲁棒性的特征向量. L2-Net 使用卷积神经网络 (CNN) 来学习从图像块到描述符的映射, 并通过端到端的训练来优化描述符的质量.

L2-Net的精髓在于它如何训练, 它设计了三个层面的Loss
1. *匹配能力约束*: 确保相似的图像块生成的描述符在L2空间中距离较近, 而不相似的图像块生成的描述符距离较远.
2. *维度相关性约束*: 通过正则化手段, 降低描述符各维度之间的相关性, 提高描述符的独立性和信息量.
3. *中间特征一致性*: 不仅最终输出要好, 中间的Feature Map也要保持这种鉴别力, 以此来约束特征提取的过程.

=== RANSAC
无论描述子多强, 特征匹配总会有错 (Outliers) 如果直接用最小二乘法拟合模型 (如单应性矩阵), 一个离群点就能毁掉整个结果. 

RANSAC (Random Sample Consensus) 的思想是 "少数服从多数, 但首先要找到那个正确的多数派".

1. *随机采样*: 随机选出计算模型所需的最少点数 (比如拟合直线只需 2 点, 单应性矩阵需 4 点)
2. *建立假设*: 假设这几个点都是内点 (Inliers), 计算出模型参数 (如直线方程, 单应性矩阵)
3. *验证假设*: 计算所有点到该模型的距离, 如果距离小于某个阈值, 则认为该点是内点, 将其加入内点集合
4. *统计票数*: 记录内点数量, 如果当前内点数量超过之前的最大值, 则更新最佳模型
5. *重复迭代*: 重复上述过程若干次 (迭代次数可预设), 最终选择内点数量最多的模型作为最终结果.

= 视觉几何
== 光学的物理基础
*薄透镜公式*
$
1/f = 1/d_o + 1/d_i
$

*景深 (Depth of Field, DoF)*: 指物体前后清晰成像的范围. 从薄透镜公式知道, 只有在一个特定平面上的物点才能在像平面上完美聚焦, 而其他距离的点在传感器上不再汇聚为一点, 而是形成一个光斑——称为弥散圆 (Circle of Confusion). 当这个光斑小到人眼或传感器无法分辨时, 我们仍认为它是 "清晰" 的, 景深就是前后允许弥散圆不超过某个阈值的物距范围.

当你用手机或相机对焦在一朵花上, 背后的树会变模糊; 而如果对焦在远处的山, 近处的草就虚了, 但如果你把光圈收得很小 (比如 f/16), 你会发现前后都变清晰了, 这就是景深变大. 光圈越小, 光线穿过镜头的锥角越窄, 弥散圆越小，模糊越不明显, 景深就越大. 反之, 大光圈 (如 f/1.4) 产生浅景深, 背景强烈虚化.

*视角 (Field of View, FoV)*: 指摄像机能够捕捉到的场景范围. 视角与焦距成反比. 焦距越短 (如28mm广角) 视角越宽; 焦距越长 (如300mm长焦), 视角越窄, 能把远处的物体拉近.
== 坐标系和变换
当我们将物理过程抽象为数学模型时, 核心任务就是建立从世界坐标系到像素坐标系的映射. 我们可以将这个过程拆解为三个阶段, 就像工厂的流水线:

=== 刚体变换 (真实世界 $->$ 相机)
首先, 我们要确定摄像机在哪里. 世界坐标系 $P_w$ 是绝对的, 而相机坐标系 $P_c$ 是相对于摄像机的位置和方向而言的. 通过一个刚体变换 (旋转矩阵 $R$ 和平移向量 $t$), 我们可以将世界坐标系中的点转换到相机坐标系:
$
P_c = R P_w + t
$
这两个参数描述了相机在空间中的位姿, 即位置和朝向. 旋转矩阵 $R$ 是一个 $3 times 3$ 的正交矩阵, 描述了相机的方向; 平移向量 $t$ 是一个 $3 times 1$ 的向量, 描述了相机的位置. 它们被称为 *外参 (Extrinsic Parameters)*, 因为它们描述的是相机相对于外部世界的关系.

=== 透视投影 (相机 $->$ 图像物理平面)
这是产生 "近大远小" 视觉效果的关键步骤. 透视投影将三维空间中的点 $P_c (X_c, Y_c, Z_c)$ 映射到二维图像平面上的点 $P_i (x_i, y_i)$. 这个过程可以通过以下公式表示:
$
x_i = f (X_c / Z_c), quad y_i = f (Y_c / Z_c)
$
这里导致了深度的非线性丢失, 也是透视失真 (如果你拍摄一条笔直的道路, 它会在远处汇聚成一个点) 的根源. 透视投影是通过相机的焦距 $f$ 来控制的, 焦距越长, 透视效果越明显.
=== 仿射变换 (图像物理平面 $->$ 像素平面)
物理成像平面上的单位是毫米 (mm), 而计算机里的图像单位是像素 (pixel). 我们需要将物理坐标映射到像素数组的行列索引 $(u, v)$ 上, 这涉及两个操作:
1. *缩放*: 由于像素的大小通常不等于毫米, 我们需要通过缩放因子 $s_x$ 和 $s_y$ 将物理坐标转换为像素坐标
2. *平移*: 图像的原点通常在左上角, 而物理成像平面的原点在中心, 我们需要加上偏移量 $(o_x, o_y)$, 即主点 (Principal Point), 来调整坐标系的位置. $o_x, o_y, f$ 以及像素尺寸等参数, 描述了相机本身的内部属性, 被称为 *内参 (Intrinsic Parameters).*

为了将上述平移、旋转、缩放和投影统一在一个优雅的数学框架下, 我们引入了齐次坐标表示法. 齐次坐标通过添加一个额外的维度, 使得所有的变换都可以表示为矩阵乘法. 具体来说, 我们将三维点 $P_w (X_w, Y_w, Z_w)$ 表示为齐次坐标 $P_w^h (X_w, Y_w, Z_w, 1)$. 同理, 二维像素点 $P_i (u, v)$ 表示为齐次坐标 $P_i^h (u, v, 1)$. 通过齐次坐标, 我们可以将整个变换过程表示为一个单一的矩阵乘法:
$
s P_i^h = K [R | t] P_w^h
$
这里的 $K$ 是相机内参矩阵, 它包含了焦距和主点位置等信息:
$
K = mat(
  delim: "[",
  f_x, 0, o_x;
  0, f_y, o_y;
  0, 0, 1
)
$
其中 $f_x = f / s_x$ 和 $f_y = f / s_y$ 分别是以像素为单位的焦距. 通过这个统一的矩阵表示, 我们可以方便地进行各种几何变换和投影计算, 这对于计算机视觉中的许多任务 (如三维重建, 相机标定等) 都至关重要.

== 畸变与简化模型
上述的针孔模型是理想化的, 现实中的透镜并不完美, 会引入各种畸变, 主要包括:
1. *径向畸变 (Radial Distortion)*: 光线在透镜边缘弯曲程度不同, 导致直线变弯. 最常见的是桶形畸变 (画面中心膨胀, 常见于广角) 和枕形畸变 (画面向内收缩, 常见于长焦).
2. *切向畸变 (Tangential Distortion)*: 由于透镜和图像传感器未完全平行, 导致图像出现倾斜或偏移. 

此外, 如果物体离相机非常远, 且物体自身的深度变化相对于距离可以忽略不计, 我们可以使用弱透视投影或仿射相机模型. 在这种情况下, 投影线近似平行, 图像的大小只与放大倍率有关, 而不再随深度剧烈变化 (透视效应消失). 这种简化模型在某些特定的工业检测或远距离监控场景下非常有用.

== 从欧氏空间到射影空间
在经典的欧氏几何中, 我们习惯了 "平行线永不相交" 的公理. 但在相机的成像世界里, 这显然是不成立的. 铁轨在远方汇聚于一点, 这种现象迫使我们要引入射影几何. 

*齐次坐标和无穷远点*: 通过引入齐次坐标, 我们可以将三维点 $(X, Y, Z)$ 表示为四维齐次坐标 $(X, Y, Z, W)$. 当 $W$ 不为零时, 我们可以通过归一化得到欧氏坐标 $(X/W, Y/W, Z/W)$. 当 $W$ 趋近于零时, 该点被认为位于无穷远处, 这就是射影空间中的 *无穷远点*. 无穷远点允许我们优雅地处理平行线相交的问题, 因为在射影空间中, 所有平行线都在某个无穷远点相交. 在射影空间中, 无穷远点和普通点没有本质区别, 它们都是空间中的一个向量, 我们可以统一地对它们进行矩阵运算.

*常见变换*:
1. 欧氏变换 (Euclidean): 最刚性, 保持长度、角度、面积. 像是一个刚体在移动.
2. 相似变换 (Similarity): 允许缩放, 保持形状 (角度) 但不保持大小. 
3. 仿射变换 (Affine): 允许平行线保持平行, 但不保持角度和长度. 像是一个橡皮筋在拉伸.
4. 射影变换 (Projective): 最一般化, 平行线不再平行, 相交于无穷远点. 此时, 只有交比 (Cross Ratio)  和 结合关系（如共线、共点） 保持不变

== 消隐点与消隐线
*消隐点 (Vanishing Point)* 是三维空间中 "方向" 在二维图像上的投影. 直觉上, 你只要往某个方向看无穷远, 视线最终汇聚的那个点就是消隐点. 这导致了一个极其重要的性质: 空间中一组平行的直线, 在图像上会交汇于同一个消隐点. 数学上, 这可以理解为直线的方向向量经过相机投影后, 落在图像平面的某个点上.

*消隐线 (Vanishing Line)* 是一组平行平面的法向量在图像上的投影. 换句话说, 如果你有一组平行的平面, 它们的交线在图像上会形成一条直线, 这条直线就是消隐线. 消隐线可以看作是无穷远平面的投影, 它包含了所有消隐点.

利用消隐点和消隐线, 我们可以推断出相机的姿态 (位置和方向), 以及场景的几何结构. 例如, 在建筑物的图像中, 我们可以通过检测墙壁的消隐点来估计相机的朝向; 也可通过一个参考物的已知尺寸, 结合消隐点计算出其他物体的实际大小.

== 单应矩阵 (Homography)
单应矩阵 $H$ 是一个 $3 times 3$ 的非奇异矩阵, 它描述了两个平面之间的射影变换关系. 数学表示为
$
  x' = H x, quad x = (u, v, 1)^T, quad x' = (u', v', 1)^T
$
这里, $x$ 和 $x'$ 分别是原始平面和目标平面上的齐次坐标点. 单应矩阵可以表示为
$
H = mat(
  delim: "[",
  h_11, h_12, h_13;
  h_21, h_22, h_23; 
  h_31, h_32, h_33
)
$
虽然 $H$ 有 9 个元素, 但由于齐次坐标的比例不变性, 实际上只有 8 个自由度 (可以通过归一化 $h_33 = 1$ 来消除一个自由度). 所以我们至少需要 4 对对应点 (每对点提供 2 个方程) 来唯一确定单应矩阵.