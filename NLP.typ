#import "@preview/showybox:2.0.4": showybox

#import "@preview/ilm:1.4.1": *
#import "@preview/finite:0.5.0": automaton
#import "@preview/finite:0.5.0"
#import "@preview/fletcher:0.5.8" as fletcher: diagram, node, edge
#import "@preview/tdtr:0.3.0" : *

#set text(font: ("Libertinus Serif", "Source Han Serif SC"))

#show heading.where(level: 1): set text(navy.lighten(0%))
#show heading.where(level: 2): set text(navy.lighten(20%))
#show heading.where(level: 3): set text(navy.lighten(40%))

#show ref: it => {
  text(purple, it)
}

#show: ilm.with(
  title: [Natural Language Processing],
  date: datetime.today(),
  author: "Tianlin Pan",
  table-of-contents: outline(depth: 2),
)

#set heading(numbering: "1.1.1")
#set page(numbering: "1")
#set text(14pt)
#show raw: set text(font: "Maple Mono NF", size: 12pt)

#let frameSettings = (
  border-color: navy,
  title-color: navy.lighten(30%),
  body-color: navy.lighten(95%),
  footer-color: navy.lighten(80%),
)

#let frameSettingsEastern = (
  border-color: eastern,
  title-color: eastern.lighten(30%),
  body-color: eastern.lighten(95%),
  footer-color: eastern.lighten(80%),
)

= 统计学习方法
== 齐夫定律
在自然语言的大规模文本数据上统计词频时, 词频与词频排名之间存在一种特殊的数学关系, 即词频与其排名的乘积约等于一个常数. 这种现象被称为 *齐夫定律(Zipf's Law)*

$
  f times r = C
$
其中 $f$ 表示词频, $r$ 表示词频排名, $C$ 是一个常数.

== 最大熵模型 (MaxEnt)
最大熵模型是一种判别式模型 (Discriminative Model), 它直接对给定输入 $x$ 时输出 $y$ 的条件概率分布 $P(y|x)$ 进行建模. 最大熵模型的基本思想是, 在所有满足已知约束条件的概率分布中, 选择熵最大的那个分布作为模型.

熵定义了随机变量的不确定性. 当熵最大时, 随机变量最不确定. 如果没有任何先验知识, 最大熵模型导向均匀分布, 体现了对所有可能情况一视同仁, 赋予同等概率的公平、合理策略.

最大熵模型的学习过程等价于求解一个条件约束下的优化问题, 模型需要满足的约束基于训练数据中提取的 *特征函数* $f_j (x, y)$.
$
  f_j (x, y) = cases(
    1\, quad "if" x "and" y "satisfy feature some condition",
    0\, quad "otherwise"
  )
$
然后我们考虑建立模型分布 $p(y|x)$:
$
  p_w (y | x) = 1/(Z_w (x)) exp (sum_(i = 1)^m w_i f_i (x, y))
$
由此我们构造约束条件为:
$
  EE_(tilde(p)) [f_j] = EE_(p) [f_j] , quad forall j \
  EE_(tilde(p)) [f_j] = sum_(x, y) tilde(p)(x, y) f_j (x, y) \
  EE_(p) [f_j] = sum_(x, y) tilde(p)(x) p(y | x) f_j (x, y)
$
这里 $tilde(p)$ 是经验分布 (训练集), $p$ 是模型分布. 这个约束条件确保模型在特征函数的期望上与训练数据一致. 模型就是要在这些约束条件下最大化熵:
$
  max H(p), quad "s.t." quad EE_(p) [f_j] = EE_(tilde(p)) [f_j] , quad forall j
$
=== 使用最大熵模型进行词汇歧义消解 (WSD)
在自然语言处理中, *词汇歧义消解 (Word Sense Disambiguation, WSD)* 是指确定一个多义词在特定上下文中所表达的确切含义的任务.

1. *特征提取*
MaxEnt WSD 模型的准确性严重依赖于特征工程的质量, 因此需要提取与目标词义相关的各种特征, 包括但不限于:
- 局部词语搭配: 目标词前后 1 或 2 个位置的特定词汇, 如 'drug' 后面的 'abuse'.
- 局部词性序列: 目标词前后 1 或 2 个位置的词性标签, 如 'drug' 前面的 'DT' (限定词).
- 共现关键词: 在一定窗口内与目标词共现的关键词, 如 'addict', 'substance' 等.
- 句法依赖关系: 目标词在句法树中的角色, 如主语、宾语等.

2. *模型训练*
使用一个经过人工词义标注的语料库进行有监督学习. 训练过程通过迭代算法 (如 GIS 或 IIS) 来估计每个特征的权重 $lambda_i$. 目标就是找到一组权重, 使得模型的期望和经验分布的期望尽可能接近, 同时最大化熵.

3. *预测与推断*
对于一个新的上下文, 提取相同的特征并计算每个可能词义的条件概率 $P(y|x)$. 选择概率最高的词义作为最终预测结果.

#showybox(
  title: "广义迭代定标 (GIS)",
  frame: frameSettings,
)[
  1. 将所有特征值 $lambda_i$ 设置为初始值 (通常为零). 这使得模型一开始假设所有的分类是等可能的.
  2. 在每次迭代中, 计算当前模型下每个特征函数的期望值 $EE_(p) [f_j]$.
  3. 更新每个特征值 $lambda_j$:
  $ lambda_j = lambda_j + (1/C) log ((EE_(tilde(p)) [f_j]) / (EE_(p) [f_j])) $
  其中 $C$ 是一个常数, 通常等于每个样本中非零特征函数的最大数量.
  4. 重复步骤 2 和 3, 直到模型收敛, 即特征期望值的变化小于预设的阈值.
]
== 条件随机场及应用
*条件随机场 (Conditional Random Fields, CRF)* 是一种用于序列标注和结构预测的判别式模型. 给定观察序列 $X = (x_1, x_2, ..., x_n)$, 模型直接计算条件概率 $P(Y|X)$, 其中 $Y = (y_1, y_2, ..., y_n)$ 是对应的标注序列. 其核心在于通过链式图结构建模序列元素间的依赖关系, 使相邻标签的转移概率显式纳入计算框架.

模型的条件概率采用指数形式定义：
$
  P(Y|X) = 1/(Z(X)) exp(sum_(j=1)^m lambda_j F_j (Y, X))
$
其中 $Z(X)$ 是归一化因子，$F_j (Y, X)$ 为全局特征函数. 这些特征分为两类:
- *状态函数* 捕捉单个位置 $(x_i, y_i)$ 的局部特征 (如当前字的词性);
- *转移函数* 描述相邻标签 $(y_(i-1), y_i)$ 间的转移约束 (如"动词后接名词" 的语法规律). 特征权重 $lambda_j$ 通过最大化训练数据的对数似然估计获得.

与最大熵模型相比, 两者虽共享指数族分布形式, 但本质差异在于预测目标. 最大熵模型针对孤立样本预测单点标签 (如词义消解), 而 CRF 优化整条序列的联合概率. 这使 CRFs 能避免局部最优陷阱, 在分词、词性标注等任务中捕捉长距离依赖.

预测阶段需寻找全局最优标注序列 $Y^*$. *Viterbi 算法* 通过动态规划高效求解: 从序列起点递推计算每个位置各标签的最优路径得分, 最终回溯得到整体最优解. 该过程的时间复杂度为 $O(n dot.c k^2)$, $k$ 为标签集大小

在中文分词任务中, CRF 将文本视为字序列, 为每个字分配词位标签: *B* (词首), *M* (词中), *E* (词尾), *S* (单字词). 模型同时利用字形特征 (如偏旁部首)、上下文窗口 (前后字符) 及转移约束 (如"B后不可接S"), 显著提升切分准确率.

= $N$ 元文法模型
== 模型定义: 基于 Markov 假设
*核心问题*: 对于一个由词序列 ($s = w_1, w_2, ..., w_n$) 组成的句子, 我们如何计算其出现的概率 $P(s)$? 这很重要, 我们假设一个符合语法, 语义自然的句子的出现概率应该更高:
- *机器翻译* 中, 我们希望选择最有可能的翻译结果: $P("Good Translation") > P("Bad Translation")$
- *语音识别* 中, 我们希望选择最有可能的词序列作为识别结果 (尤其针对同音词): $P("I scream at you") > P("Ice cream at you")$

一个句子的联合概率可以被分解为条件概率的乘积:
$
  P(w_1, w_2, ..., w_n) & = P(w_1) P(w_2 | w_1) \
                        & P(w_3 | w_1, w_2) dots.c P(w_n | w_1, w_2, ..., w_(n-1))
$

但是, 直接估计这些条件概率是不可行的, 因为数据稀疏问题使得大部分长条件概率无法从有限语料中可靠估计. 为了解决这个问题, 我们引入 *$N$ 元文法模型 (N-gram Language Model)*, 通过 *马尔可夫假设 (Markov Assumption)* 简化条件概率, 即当前词的出现只依赖于前面 $N-1$ 个词:
$
  P(w_i | w_1, w_2, ..., w_(i-1)) approx P(w_i | w_(i-N+1), ..., w_(i-1))
$
由此我们可以将句子的概率近似表示为:
$
  P(w_1, w_2, ..., w_n) approx product_(i=1)^n P(w_i | w_(i-N+1), ..., w_(i-1))
$

根据这种标准, 我们可以定义不同阶数的 $N$ 元模型:
- *一元模型 (Unigram Model, N=1)*: 假设每个词的出现独立于其他词, 即 $P(w_i | w_1, ..., w_(i-1)) = P(w_i)$. 句子概率为各词概率的乘积:
$ P(w_1, w_2, ..., w_n) = product_(i=1)^n P(w_i) $
- *二元模型 (Bigram Model, N=2)*: 假设每个词的出现只依赖于前一个词, 即 $P(w_i | w_1, ..., w_(i-1)) = P(w_i | w_(i-1))$. 句子概率为各词在前词条件下的概率乘积:
$ P(w_1, w_2, ..., w_n) = P(w_1) product_(i=2)^n P(w_i | w_(i-1)) $
- *三元模型 (Trigram Model, N=3)*: 假设每个词的出现依赖于前两个词, 即 $P(w_i | w_1, ..., w_(i-1)) = P(w_i | w_(i-2), w_(i-1))$. 句子概率为各词在前两词条件下的概率乘积:
$ P(w_1, w_2, ..., w_n) = P(w_1) P(w_2 | w_1) product_(i=3)^n P(w_i | w_(i-2), w_(i-1)) $

我们可以引入 BOS (Beginning of Sentence) 和 EOS (End of Sentence) 标记来处理句子开头和结尾的边界条件. 例如, 对于二元模型, 我们令 $w_0 = "BOS"$, $w_(n+1) = "EOS"$, 则句子概率表示为:
$
  P(w_1, w_2, ..., w_n) = product_(i=1)^(n+1) P(w_i | w_(i-1))
$

== 参数估计: 最大似然估计 (MLE)
模型参数的概率值, 可以通过其在大型训练语料库 (training corpus) 中的相对频率来估计
$
  P(w_i | w_(i-N+1), ..., w_(i-1)) = C(w_(i-N+1), ..., w_(i-1), w_i) / C(w_(i-N+1), ..., w_(i-1))
$
这里 $C(w_(i-N+1), ..., w_(i-1), w_i)$ 是在语料库中观察到的词序列 $(w_(i-N+1), ..., w_(i-1), w_i)$ 的出现次数, 而 $C(w_(i-N+1), ..., w_(i-1))$ 是前缀词序列 $(w_(i-N+1), ..., w_(i-1))$ 的出现次数.

由此我们就可以用 MLE 方法估计 $N$ 元模型的所有条件概率参数, 使得训练语料库的似然函数最大化.

== 未登录词 (Out-of-Vocabulary, OOV) 和数据稀疏问题

如果一个词序列在训练语料库中未出现, 则其计数为零, 导致条件概率估计为零. 这个是问题是灾难性的, 因为只要有一个词的条件概率为零, 整个句子的概率就为零.
$
  P(w_1, w_2, ..., w_n) = P(...) times P(...) times 0 times P(...) = 0
$
这是不可接受的, 因为测试数据中总能遇到训练数据中未见过的词序列.

为了解决这个问题, 我们需要使用 *平滑技术 (Smoothing Techniques)* 来调整概率估计, 给未见过的词序列分配一个非零概率.
=== 加法平滑 (Additive Smoothing)
也称为拉普拉斯平滑 (Laplace Smoothing), 通过在所有计数上加一个小的常数 (通常为 1) 来避免零计数.
$
  P (w_i | w_(i-N+1), ..., w_(i-1)) = (C(w_(i-N+1), ..., w_(i-1), w_i) + 1) / (C(w_(i-N+1), ..., w_(i-1)) + abs(V))
$
其中 $abs(V)$ 是词汇表的大小.
=== 古德-图灵平滑 (Good-Turing Smoothing)
调整频次为 $r$ 的事件的估计频次为 $r^* = (r + 1) (N_(r+1) / N_r)$, 其中 $N_r$ 是频次为 $r$ 的事件数量.
=== 后备法 (Backoff)
高阶 $N$-gram 模型更精确但是数据稀疏问题更严重, 低阶 $N$-gram 模型更平滑但是信息量较少. 后备法结合了两者的优点

- 如果一个高阶 $N$-gram 的计数非零, 则使用其概率估计.
- 否则, 退回到低阶 $N$-gram 的概率估计, 并进行适当的归一化.
$
  P_"BO" (w_i | w_(i - 2), w_(i - 1)) = cases(
    P (w_i | w_(i - 2), w_(i - 1)) \, quad "if" "count" > 0,
    alpha P_"BO" (w_i | w_(i - 1)) \, quad "otherwise"
  )
$

无论采用什么平滑方法, 目标都是确保所有可能的词序列都有一个合理的非零概率估计, 需要满足概率归一化条件: $sum_(w_i) P(w_i | w_(i-N+1), ..., w_(i-1)) = 1$.

= 词法分析
== 词法分析的四大基石
- *词语切分* (Word Segmentation): 将文本拆分成有意义的词序列.
- *命名实体识别* (Named Entity Recognition): 识别文本中的命名实体, 如人名、地名、组织名等.
- *子词压缩* (Subword Tokenization): 将词划分为更小的子词单元, 以处理未登录词 (OOV) 和提高模型的泛化能力.
- *词性标注* (Part-of-Speech Tagging): 为每个词分配词性标签, 描述词在句中的语法角色.

== 词语切分
=== 核心挑战
- 切分的规范问题: 词和短语的界限非常模糊. 比如说 "花草" 到底是一个词, 还是两个词.
- 切分的歧义问题: 一个词可以有多个切分方式. 比如说 "门把手弄坏了" 可以被切分为 "门把手 | 弄 | 坏 | 了", 也可以切分为 "门 | 把手 | 弄 | 坏 | 了".
- 未登录词: 词典中未收录的词是分词错误的一个主要来源.

=== 最大匹配法 (Max-Matching)
最大匹配法的核心思想是, 从字符串的一段开始, 与词典进行匹配, 力求每次能匹配到最长的词. 这里可以有两个方向的选择:
- *正向最大匹配 (Forward Max-Matching)*: 从字符串的开头开始匹配, 一直向后推进.
- *逆向最大匹配 (Backward Max-Matching)*: 从字符串的结尾开始匹配, 一直向前推进.

比如说对于句子 "他是研究生物化学的一位科学家":
- 正向匹配的结果是: "他 | 是 | 研究生 | 物化 | 学 | 的 | 一位 | 科学家" 
- 逆向匹配的结果是: "科学家 | 一位 | 的 | 化学 | 生物 | 研究 | 是 | 他" 

最大匹配法的优点是无需训练, 速度快; 但是缺点是无法处理歧义和未登录词问题.

=== 统计切分
可以分成两种范式:
1. *基于 $N$ 元文法模型的方法 (生成式)*: 将分词视为一个生成过程, 一个句子 $P$ 的最佳切分 $S$ 是使其出现概率最大的切分方案:
$
  S = arg max_S P(S) = arg max_S product_(i=1)^m P(w_i | w_(i-N+1), ..., w_(i-1))
$

2. *由字构词模型 (判别式)*: 把分词看作为对句子中的每一个字的分类问题, 为每一个字打上一个位置标签 (词首 B, 词中 M, 词尾 E, 单字词 S). 然后使用分类模型 (如条件随机场 CRF) 来预测每个字的标签序列.

=== 如何衡量分词的好坏
*正确率* $P$: 被正确切分的词数占系统切分出的词数的比例.
$
  P = ("Number of Correctly Segmented Words") / ("Total Number of Words Segmented by System")
$

*召回率* $R$: 被正确切分的词数占参考标准 (Ground Truth) 中词数的比例.
$
  R = ("Number of Correctly Segmented Words") / ("Total Number of Words in Ground Truth")
$

*F1 值*: 综合考虑正确率和召回率的调和平均数.
$
  "F1" = 2 / (1/P + 1/R) = (2P R) / (P + R)
$

== 命名实体识别 (NER)
命名实体识别 (Named Entity Recognition, NER) 是自然语言处理中的一项重要任务, 旨在从文本中识别和分类具有特定意义的实体, 如人名 (PER), 地名 (LOC), 组织名 (ORG) 等.

=== 核心挑战
- *边界模糊*: 比如说 "中国人民大学" 是一个整体的组织名, 但其中包含了地名 "中国" 和普通名词 "人民".
- *兼类严重*: 同一个实体在不同上下文中可能属于不同类别. 例如 "苹果" 可以指代水果 (ORG) 也可以指代公司 (ORG).

=== 序列标注法
将 NER 任务视为一个序列标注问题, 为文本中的每个词分配一个标签. 常用的标签集为 BIO (Begin, Inside, Outside), 它使用 B-Type 表示实体的开始, I-Type 表示实体的内部, O 表示非实体词. 例如, 对于句子 "张宁在山西队", 其标注结果为:

张/B-PER | 宁/I-PER | 在/O | 山西/B-ORG | 队/I-ORG

== 字词压缩: 应对词表爆炸和 OOV
=== 动机
我们可以用词或者字作为自然语言的基本单位.

如果以词为单位, 则语义信息最为丰富, 但词表规模巨大, 导致模型参数过多, 训练困难, 并且容易遇到未登录词 (OOV) 问题和长尾分布问题.

如果以字为单位, 则词表规模小, 模型参数少, 不存在未登录词问题, 但语义信息有限, 可能无法捕捉到完整的词义. 单个字的歧义程度较高, 影响模型性能.

为了解决这些问题, 我们可以采用 *子词压缩 (Subword Tokenization)* 技术, 将词划分为更小的 *子词* 单元. 这样既能减少词表规模, 又能保留一定的语义信息. 我们希望字词是一种介于字和词之间的最佳粒度.

=== 字节对编码 (Byte-Pair Encoding, BPE)
BPE 是一种基于频率的子词划分方法, 其核心思想是通过迭代地合并最频繁出现的字符对来构建子词词表. 具体步骤如下:
1. 初始化: 将训练语料中的每个词拆分为单个字符, 并统计所有字符对的频率.
2. 迭代合并: 找到频率最高的字符对, 将其合并为一个新的子词. 更新语料中的词表示, 并重新统计字符对频率.
3. 重复步骤 2, 直到达到预设的子词数量或频率阈值.

举一个例子, 比如说我们最初的语料库是
$
  {"'a a a b d'": 1, "'a a a b a b c'": 1}
$

Step 1: 统计出现频率最高的字符对, 这里是 "a a", 出现了 4 次. 合并 "a a" 为 "aa", 则新的语料库是
$
  {"'aa a b d'": 1, "'aa a b a b c'": 1}
$

Step 2: 继续统计字符对, 现在 "a b" 出现频率最高, 出现了 3 次. 合并 "a b" 为 "ab", 则新的语料库是
$
  {"'aa ab d'": 1, "'aa ab ab c'": 1}
$

Step 3: 继续统计字符对, 现在 "aa ab" 出现频率最高, 出现了 2 次. 合并 "aa ab" 为 "aaab", 则新的语料库是
$
  {"'aaab d'": 1, "'aaab ab c'": 1}
$

通过这种方式, 我们可以逐步构建一个子词词表, 并将原始词表示为子词的组合.

== 词性标注 (POS Tagging)
词性标注任务旨在为文本中的每个词分配一个正确的词性标签, 描述其在句子中的语法角色. 但是一个词的词性高度依赖于其上下文环境, 因此词性标注通常被视为一个序列标注问题.

所以我们一般使用隐马尔可夫模型 (HMM) 或条件随机场 (CRF) 等序列模型来进行词性标注. 这些模型能够捕捉词与其上下文之间的依赖关系, 提高标注的准确性.

= 语义分析
== 语义分析的核心挑战: 歧义问题
句法分析为我们提供了句子的结构, 但要真正理解其含义, 我们必须应对语言中普遍存在的歧义和语境依赖性.

*词义悖论*: 相同的句法结构, 甚至看似相反的词, 却可以表达相同的意思. 如 "中国队大败美国队" 和 "中国队大胜美国队" 都可以表示中国队在比赛中获胜. 

*语境依赖*: "夏天能穿多少穿多少" 和 "冬天能穿多少穿多少", 这两句话在不同的季节下有不同的含义.

*知识依赖*: "中国足球谁也打不过" 和 "中国乒乓球谁也打不过", 理解这两个例子需要对中国足球和乒乓球的历史和现状有一定了解.

== 语义网络 (Semantic Network)
语义网络是一种通过有向图来表达知识和描述语义的方法. 它由代表实体或概念的 *节点*, 和代表实体间关系的 *边* 组成. 节点可以是具体的事物 (如 "苹果"), 抽象的概念 (如 "水果"), 或其他语义单位 (如 "吃"). 边则表示节点之间的语义关系 (如 "是", "属于", "包含").

#diagram(
	node-defocus: 0,
	spacing: (1cm, 2cm),
	edge-stroke: 2pt,
	mark-scale: 70%,
	node-fill: luma(97%),
	node-outset: 3pt,
	node((-2,-1), "老虎"),
	node((2,-1), "鸟"),
	node((4.5,-1), "翅膀"),
	node((0,0), "动物"),
	node((-5,1), "桌面"),
	node((-2,1), "桌子"),
	node((2,1), "鱼"),
	node((4.5,1), "水"),
	{
		let quad(a, b, label, paint, ..args) = {
			paint = paint.darken(25%)
			edge(a, b, text(paint, label), "-|>", stroke: paint, label-side: center, ..args)
		}

		quad((-2,-1), (0,0), "IS-A (是一种)", blue)
    quad((2,-1), (0,0), "IS-A (是一种)", blue)
    quad((-5,1), (-2,1), "PART-OF", orange)
    quad((-2,1), (0,0), "IS-A (是一种)", blue)
    quad((2,1), (0,0), "IS-A (是一种)", blue)
    quad((2,1), (4.5,1), "LIVE-IN", green, label-pos: 0.4)
    quad((2, -1), (4.5,-1), "HAVE", purple, label-pos: 0.4)
	},
)

== 语义角色标注 (Semantic Role Labeling, SRL)
=== 概述
SRL 旨在识别句子中的谓词 (通常为动词), 并分析句子中的其他成分与该谓词之间的语义关系. 比如说谁 (Who) 在何时 (When) 何地 (Where) 对谁 (Whom) 做了什么 (What) 以何种方式 (How).

比如说对于如下句子:
#align(center)[
  #set text(weight: "bold")
  #text("他们 (Agent)", blue) #text("昨天 (Time)", orange) #text("在北京 (Location)", green) \ #text("讨论 (Predicate)", red) 了 #text("方案 (Patient)", purple)
]

SRL 不试图理解整个世界, 而是专注与精确解析句子中的语义结构, 以支持更高层次的自然语言理解任务.

=== 命题库 (PropBank)
要训练统计 SRL 模型, 我们需要大规模的标注数据. PropBank 在宾夕法尼亚树库 (Penn Treebank) 的基础上, 为每个动词添加了语义角色标注. 它定义了一套通用的角色标签, 如 ARG0 (施事), ARG1 (受事), ARG2 (工具/方式), 以及修饰语 (AM-LOC, AM-TMP 等). PropBank 的目标是让具有相似语义角色的成分或得一致的标签, 以便模型能够学习到通用的语义模式.

比如说对于 "It operates stores mostly in Iowa and Nebraska." 

#align(center)[
  #set text(weight: "bold", purple)
  #tidy-tree-graph(
    spacing: (20pt, 20pt),
    text-size: 13pt, 
    node-inset: 6pt
  )[
    - S
      - NP
        - ARG0
          - "It"
      - VP
        - Predicate
          - "operates"

        - NP
          - ARG1
            - "stores"
        - PP
          - ARGM-LOC
            - "mostly in Iowa and Nebraska" 
  ]
]

其中
- *S* 代表句子 (Sentence)
- *NP* 代表名词短语 (Noun Phrase)
- *VP* 代表动词短语 (Verb Phrase)
- *PP* 代表介词短语 (Prepositional Phrase)

=== 核心论元
在 PropBank 中, 每个动词都有一个对应的 *论元框架 (Argument Frame)*, 定义了该动词的核心论元及其语义角色. 
- *ARG0*: 通常表示动作的施事 (Agent) 或执行者.
- *ARG1*: 通常表示动作的受事 (Patient) 或主题 (Theme).
- *ARG2*: 通常表示工具 (Instrument), 方式 (Manner), 或受益者 (Beneficiary).
- *ARG3*: 动作的起点 (Source)
- *ARG4*: 动作的结束点 (Goal)

=== 辅助性/修饰性论元
除了核心论元外, PropBank 还定义了一些辅助性或修饰性论元, 用于描述动作的时间 (AM-TMP), 地点 (AM-LOC), 方式 (AM-MNR), 原因 (AM-CAU) 等附加信息.
- *AM-TMP*: 描述动作发生的时间.
- *AM-LOC*: 描述动作发生的地点.
- *AM-MNR*: 描述动作的方式或手段.
- *AM-ADV*: 描述动作的频率、程度等附加信息.

=== 基于短语结构树的 SRL 标注
SRL 标注通常基于句子的短语结构树 (Phrase Structure Tree), 通过识别与谓词相关的短语节点来分配语义角色. 它的核心过程是:
1. *候选论元剪枝 (Puning)*: 从谓词节点开始向上遍历短语结构树, 收集可能的论元短语节点.
2. *分类 (Classification)*: 使用分类模型 (如最大熵模型, 条件随机场等) 为每个候选论元分配语义角色标签.

比如说对于下面的句子: "外商投资企业发挥了作用."

#align(center)[
  #set text(weight: "bold", purple)
  #tidy-tree-graph(
    spacing: (20pt, 20pt),
    text-size: 13pt, 
    node-inset: 6pt
  )[
    - S
      - NP
        - ARG0
          - "外商投资企业"
      - VP
        - Predicate
          - "发挥了"
        - NP
          - ARG1
            - "作用"
  ]
]

我们的标注过程为:
1. 从谓词 "发挥了" 开始, 向上遍历短语结构树, 识别出候选论元短语节点 "外商投资企业" 和 "作用".
2. 使用分类模型为 "外商投资企业" 分配 ARG0 (施事), 为 "作用" 分配 ARG1 (受事).

=== 基于依存关系的 SRL 标注
除了基于短语结构树的方法外, 依存关系 (Dependency Relations) 也可以用于 SRL 标注. 依存关系直接表示词与词之间的语法关系, 可以更直观地捕捉谓词与其论元之间的联系.

比如说我们有如下句子: "警方正在详细调查事故原因", 其依存关系图如下:

#diagram(
  node-defocus: 0,
  spacing: (2cm, 2cm),
  edge-stroke: 2pt,
  mark-scale: 70%,
  node-fill: luma(97%),
  node-outset: 3pt,
  let pos_core = (1.5, 0),
  node(pos_core, "调查"),
  node((-1,-1), "警方"),
  node((0,-1), "正在"),
  node((1,-1), "详细"),
  node((2,-1), "事故"),
  node((3,-1), "原因"),
  {
    let quad(a, b, label, paint, ..args) = {
      paint = paint.darken(25%)
      edge(a, b, text(paint, label), "-|>", stroke: paint, label-side: center, ..args)
    }

    quad(pos_core, (-1,-1), "sbj", blue)
    quad(pos_core, (0,-1), "vmod", orange)
    quad(pos_core, (1,-1), "vmod", green)
    quad(pos_core, (2,-1), "obj", red)
    quad(pos_core, (3,-1), "obj", purple)
  },
)

因此我们可以把 "警方" 标注为 ARG0 (施事), "事故原因" 标注为 ARG1 (受事), "正在详细" 标注为 AM-MNR (方式).

=== SRL 作为序列标注问题
SRL 也可以被视为一个序列标注问题, 类似于词性标注或命名实体识别. 我们可以为句子中的每个词分配一个语义角色标签, 并使用序列模型 (如条件随机场 CRF, 循环神经网络 RNN 等) 来捕捉词与其上下文之间的依赖关系.

= 神经网络与神经语言模型
传统 n-gram 模型长期面临着数据稀疏的困境. 当某个词对未在训练语料中出现时, 模型便无法给出合理的概率预测; 更重要的是, 它将词语视为孤立的离散符号, 无法理解 "枯燥" 与 "乏味" 在语义上的亲缘关系 . 为了打破这种局限, 分布式表示 (Distributed Representation) 应运而生. 它将词语投射到低维, 稠密的连续实数空间中, 使得语义相近的词在向量空间中也彼此靠近.

== FNN
== RNN
== LSTM

= 文本表示
== 向量空间模型 (VSM)
=== 基本概念
向量空间模型的基本逻辑是: 将文档看作是由一系列相互独立的 *特征项* (通常是词组) 组成的集合. 如果我们把词典中每一个不重复的词都视为一个坐标轴, 那么每一篇文档就可以被表示为这个高维空间中的一个向量.

形式化地讲, 每一维坐标轴代表一个独立的词项 $t_i$, 而这些坐标轴在逻辑上被假设为相互正交的. 因此, 一篇文档 $d$ 可以表示为一个 $n$ 维向量:
$
  d = (w_1, w_2, ..., w_n)
$
其中 $w_i$ 是词项 $t_i$ 在文档 $d$ 中的权重. 这种表示方式使得我们可以利用向量空间中的各种数学工具来分析和比较文档, 如计算文档间的相似度 (通常使用余弦相似度), 进行聚类或分类等.

=== 词袋模型 (Bag-of-Words, BOW)
词袋模型是向量空间模型中最简单且最常用的一种形式. 它忽略了词语在文档中的顺序和语法结构, 只关注词语的出现频率. 在 BOW 模型中, 文档被表示为一个词频向量, 其中每个维度对应一个词项, 值为该词项在文档中出现的次数.

例如, 假设我们的词典包含以下词项: ["猫", "狗", "鱼"]. 那么文档 "猫和狗" 可以表示为向量 $(1, 1, 0)$, 因为 "猫" 和 "狗" 各出现了一次, 而 "鱼" 没有出现.

=== 权重计算
在向量空间模型中, 词项的权重 $w_i$ 可以通过多种方式计算, 以反映其在文档中的重要性. 常见的权重计算方法包括:
- *布尔权重 (Boolean Weighting)*: 如果词项在文档中出现, 则 $w_i = 1$; 否则 $w_i = 0$.
- *词频 (Term Frequency, TF)*: 直接使用词项在文档中的出现次数作为权重.
$
  "TF"(t_i, d) = "count"(t_i, d)
$
- *逆文档频率 (Inverse Document Frequency, IDF)*: 通过衡量词项在整个语料库中的稀有程度来调整权重. 计算公式为:
$
  "IDF"(t_i) = log(N / "df"(t_i))
$
这里 $N$ 是语料库中的总文档数, 而 $"df"(t_i)$ 是包含词项 $t_i$ 的文档数.
- *TF-IDF*: 结合词频和逆文档频率, 以平衡词项在单个文档中的重要性和在整个语料库中的普遍性. 计算公式为:
$
  "TF-IDF"(t_i, d) = "TF"(t_i, d) times "IDF"(t_i)
$

=== 规范化
由于文本长度不一会导致项频天然的差异 (长文本词频通常更高), VSM 往往需要进行文本长度规范化.

- *L1 规范化*: 将向量的每个分量除以向量的 L1 范数 (即各分量绝对值之和), 使得规范化后的向量各分量之和为 1.
- *L2 规范化*: 将向量的每个分量除以向量的 L2 范数 (即各分量平方和的平方根), 使得规范化后的向量长度为 1.
- *最大词频规范化*: 将向量的每个分量除以向量中的最大分量值, 使得规范化后的向量的最大分量为 1.

=== 局限性
虽然 VSM 简单且可计算, 但它也存在明显弱点: 
- *特征稀疏性*: 由于词典规模巨大, 文档向量通常非常稀疏, 导致计算效率低下.
- *缺乏语义相似性*: VSM 假设词项之间相互独立, 无法捕捉词语间的语义关系. 例如, "汽车" 和 "车" 在 VSM 中被视为完全不同的维度, 尽管它们在语义上高度相关.
- *忽略词序和上下文*: VSM 忽略了词语在文档中的顺序和上下文信息, 可能导致语义理解的缺失.

== 表示学习模型
表示学习通过模型自动学习文本的低维、稠密实数向量表示. 在自然语言处理中, 表示学习模型主要分为以下两类代表性方法:
- *文本概念表示模型*: 以主题模型 (如 LDA, 潜在狄利克雷分布)为代表, 旨在挖掘文本中隐含的主题或概念, 将文本表示为主题的分布向量.
- *分布式表示学习模型*: 通过深度学习模型优化目标函数, 在分布式向量空间中学习文本的低维实数向量表示.

=== 词汇表示学习 (Word Embedding)
- *C&W*: 这是词汇表示学习的开创性工作. 它的核心思路是利用上下文词预测当前词, 在训练时要求正确词组序列的得分 (score) 必须高于将中心词随机替换后的负样本得分, 模型通过最小化目标函数, 使正样本的分值比负样本至少高出 1 分

- *CBOW (Continuous Bag-of-Words)*: 该模型通过上下文词来预测中心词. 给定一个词的上下文窗口, 模型试图最大化在该上下文下预测中心词的概率. 这种方法能够有效捕捉词语之间的语义关系.

- *Skip-Gram*: 该模型与 CBOW 相反, 它通过中心词来预测其上下文词. 给定一个中心词, 模型试图最大化在该中心词下预测其上下文词的概率. Skip-Gram 在处理低频词时表现更好, 能够学习到更丰富的语义信息.
=== 短语表示学习 (Phrase Embedding)
- *词袋法*: 将短语视为词向量的集合, 通过简单的数学运算进行组合. 具体计算方式包括:
  - 平均法: 计算短语中所有词向量的平均值作为短语向量.
  - 加权平均法: 根据词频或 TF-IDF 权重计算加权
  - 最大值法: 对短语中所有词向量的每一维取最大值, 合并成一个新的向量.

- *递归自编码器 (Recursive Autoencoder, RAE)*: 该方法通过递归地将词向量组合成短语向量. 假设一个短语由 $n$ 个词组成, 模型利用二叉树结构, 每次将相邻的两个向量合并成一个新的向量, 直到得到整个短语的表示. RAE 属于自监督学习, 在合并向量的同时, 模型会尝试从合并后的向量重建原始词向量, 以最小化重建误差.
=== 句子表示学习 (Sentence Embedding)
- *段落向量模型 (Paragraph Vector, PV)*: 该模型通过引入一个全局的段落向量, 与词向量一起用于预测句子中的词. 段落向量捕捉了句子的整体语义信息, 能够更好地表示句子的含义. 由此我们可以构造 PV-DM (CBOW 的拓展) 和 PV-DBOW (Skip-Gram 的拓展) 两种变体.
- *Skip-Thought 模型*: 该模型通过编码一个句子, 来预测其前后相邻的句子. 这种方法利用了句子之间的上下文关系, 能够学习到更丰富的句子表示. Skip-Thought 模型通常使用循环神经网络 (RNN) 或长短期记忆网络 (LSTM) 作为编码器和解码器.
- *基于RNN/LSTM的句子编码器*: 该方法使用 RNN 或 LSTM 来逐词处理句子, 并将最后一个时间步的隐藏状态作为句子的表示. 这种方法能够捕捉句子中的顺序信息和上下文依赖关系, 提供更准确的句子表示.
=== 文档表示学习 (Document Embedding)
- *基于句子语义组合的表示法*: 该方法首先使用句子表示学习模型 (如 Skip-Thought 或 RNN/LSTM 编码器) 来获取文档中每个句子的向量表示. 然后, 通过简单的数学运算 (如平均、加权平均或最大值) 或者更复杂的注意力机制, 将这些句子向量组合成一个整体的文档向量. 这种方法能够捕捉文档中的局部和全局语义信息, 提供更丰富的文档表示.
- *层次化自编码器 (Hierarchical Autoencoder)*: 该方法通过分层结构来学习文档表示. 首先, 使用句子级别的自编码器来获取每个句子的向量表示. 然后, 将这些句子向量作为输入, 使用文档级别的自编码器来学习整个文档的表示. 这种层次化的方法能够更好地捕捉文档中的语义层次结构, 提供更准确的文档表示.


